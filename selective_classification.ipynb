{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt9Bt291J76u"
   },
   "source": [
    "# Selective Classification Can Magnify Disparities Across Groups\n",
    "\n",
    "#### [Jones, Sagawa, Koh, Kumar, & Liang (2021)](https://arxiv.org/pdf/2010.14134.pdf)\n",
    "\n",
    "#### AM207: Advanced Scientific Computing: Stochastic Methods for Data Analysis, Inference, and Optimization\n",
    "\n",
    "#### Team: Jamelle Watson-Daniels, Shirley Wang, Bridger Ruyle, Paul Tembo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement \n",
    "#### What is the problem the paper aims to solve?\n",
    "\n",
    "Selective classification is a common approach within the machine learning field and refers to model absentions in cases of high uncertainty. By making predictions only when the model is highly confident in the outcome (for example, $p(y | x) > 0.5 + \\tau$ for binary classifiers where $\\tau$ is a threshold chosen by the model user), the model can improve average and out-of-distribution accuracies. This method may be particularly desirable in situations for which errors are costly, such as healthcare. In these cases, when a model is uncertain, decision-making can be deferred to a human expert (e.g., physician). Selective classification also has nice properties, such as improving average and out-of-distribution accuracies. In this paper, the authors demonstrate that this phenomenon is theoretically guaranteed for models with confidence distributions that are left-log concave and right skewed (i.e. skewed towards correct predictions) and that accurately predict the outcome $>50\\%$ of the time.\n",
    "\n",
    "However, in this paper, the authors also report a cautionary finding, such that selective classification can *magnify* existing disparities between groups in a population. Specifically, they report that for models which do not accurately predict the outcome $>50\\%$ of the time, or for subgroups in larger models whose outcomes are poorly predicted, selective classification can *decrease* predictive accuracy. This phenomenon is “an insidious failure mode” because selective classification will increasingly make wrong predictions with greater confidence, particularly for groups with poor accuracy at full coverage, thus magnifying between-group disparities. Even for subgroups that are accurately predicted $>50\\%$ of the time, selective classification improves accuracy for these groups at a slower rate compared to the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context, Existing Work, and Contribution\n",
    "#### Why is this problem important? What has been done in the literature, and what gap does the paper fill?\n",
    "\n",
    "Selective classification seems to be a natural and logical approach for real-life, risk-adverse applications. For instance, in the healthcare field, consider a suicide prediction model that aims to classify patients at high or low risk for attempting suicide in the next week. These types of models are highly desirable in clinical psychiatry settings (e.g., emergency departments, inpatient psychiatric units), and it is crucial to help healthcare providers make decisions about whether to discharge someone from the hospital or keep them in for longer. In this case, if a model is uncertain about a prediction, it would be better for the model to abstain and defer decision-making to a healthcare professional (e.g., therapist, psychiatrist). \n",
    "\n",
    "However, if selective classification can exacerbate inter-group disparities in model performance, this raises serious concerns for equity. Groups that are already underserved by our existing systems may be further disadvantaged and be at highest risk for harm by an incorrect prediction. For instance, racial and gender minorities are already underrepresented in healthcare datasets and have lower accuracies than the model average. Model bias and group disparities are major issues in these fields, and if selective classification has the potential for *exacerbating* these disparities, it is crucial to obtain a better understanding of (1) when these disparities might be exacerbated and (2) potential solutions.\n",
    "\n",
    "To date, existing work has demonstrated that models can latch onto spurious correlations in medical images (e.g., chest x-ray) and fail to appropriately detect illness for patients who may be the most at-risk (Oakden-Rayner, 2020). However, little is known about how such spurious correlations may interact with selective classification to magnify disparities, and the present paper (Jones et al., 2021) investigates this problem on five datasets that are known to develop spurious correlations during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Technical Content\n",
    "#### What are the high-level ideas and relevant technical details?\n",
    "\n",
    "In this paper, Jones and colleagues (2021) demonstrate that selective classification can magnify group disparities across five independent datasets that tend to develop spurious correlations during model training. In selective classification, a model abstains from making a prediction when its confidence $\\hat{c}$ is greater than or equal to some threshold $\\tau$, where confidence in predicting a given class $\\hat{c}(x)$ is given by: \n",
    "\n",
    "$$\\hat{c}(x) = \\frac{1}{2} \\log \\Big( \\frac{\\hat{p}(\\hat{y}(x) | x)}{1 - \\hat{p}(\\hat{y}(x) | x)} \\Big)$$\n",
    "\n",
    "As shown in **Figure 1**, for a selective classifier, the margin distributions (on the left) and accuracy-coverage curves (on the right) show a discrepancy between the average and worst group performance.\n",
    "\n",
    "<img src = 'figs/fig1.png' style = \"width: 700px;\">\n",
    "\n",
    "**Figure 1**. Left plot: given a threshold $\\tau$, a selective classifier makes incorrect classifications when predictions $\\hat{y}(x) \\leq -\\tau$, abstains from making a prediction when $-\\tau < \\hat{y}(x) < \\tau$, and makes a correct classification when $\\hat{y}(x) \\geq \\tau$. Right plot: While average accuracy from a selective classifier *increases* as more abstensions are made (i.e., average coverage/proportion of predicted points goes down), accuracy for the worst group *decreases*. Likewise, while average accuracy from a selective classifier *decreases* as fewer abstensions are made, accuracy for the worst group *increases*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments \n",
    "#### What types of experiments were performed, and do results prove the claims?\n",
    "\n",
    "The authors demonstrate the above effect across five datasets, including image and text datasets, as shown in **Figure 2**. From these images, we can see that the margin distributions tend to be shifted towards the left (i.e., less accurate predictions) for the worst group compared to the average. In other words, the selective classifier often confidently predicts incorrect labels for the worst group. In addition, the accuracy-coverage curves show that as more abstensions are made, the average accuracy tends to increase while the worst group accuracy tends to decrease.\n",
    "\n",
    "<img src = 'figs/fig2.png' style = \"width: 700px;\">\n",
    "\n",
    "This effect is perhaps most apparent for the `CelebA` dataset, which is a large image database of celebrities (Liu et al., 2015). In this case, the prediction task was to classify the hair color of celebrities (blond vs. non-blond hair), with the spurious attribute of gender, such that women were more likely to be blond. In this case, blond males were the \"underrepresented\" group in the dataset, experiencing the worst performance, due to class imbalance (representing only 8.5% of training examples) and the spurious correlation between gender and hair color. \n",
    "\n",
    "In addition to these five experiments, in the present paper, Jones and colleagues (2021) discuss a solution to mitigate this problem by training the model using group distributionally robust optimization (DRO), which minimizes the *worst-group training loss* instead of the *overall* training loss for the dataset. Using this method, the confidence distribution of the worst-group more closely matches the distribution of the overall model. Thus, DRO models allow selective classification to affect both subgroups and the overall data equally. Of note, this approach requires that group labels are known during training (which may not always be the case in real-world application), though the labels can be removed for testing/deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Our Methods - Replication and Extension \n",
    "#### How did we reproduce this paper's results, and what methods did we use? \n",
    "\n",
    "Jones and colleagues (2021) report that selective classification can magnify existing group disparities particularly in the presence of spurious correlations demonstrating this behavior across five datasets. In this project, we examine the following question: what are the dataset characteristics the lead to the increased disparities? Because the paper notes the importance of spurious correlations, we bring our attention to varying levels of correlations. We consider the image classification setting and generate synthetic datasets with varying degrees of spurious correlations. Then, we analyze the margin distributions and the selective accuracy of both the overall dataset and the worst group. Ultimately, we compare these results for the varying degrees of spurious correlations to make conclusions about the importance of the dataset characteristics in obtaining the specific magnifying group disparities result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Generating Synthetic Data\n",
    "\n",
    "The paper claims to consider datasets known to have spurious correlations. The image datasets used in the paper contain over 100,000 examples and are not practical for use in this project analysis. Therefore, we generate synthetic datasets derived from the MNIST dataset where we add spurious correlations of varying degrees. \n",
    "\n",
    "Specifically, we download the `mnist` dataset and select two numbers for the binary classification task. We introduce spurious attributes as filled in pixels added to the empty space on the upper left hand corner of a given image. These filled pixels can be thought of as boxes that will be correlated with one of the labels. In essence, a given label might then be correlated not only with the original written digit in the `mnist` dataset, but also with whether or not the image contains this added black box. In the training dataset, we add these black boxes to a percentage of the images with each label $Y \\in {0,1}$. To mimic the characteristics of the `celebA` dataset used in the paper, we keep the percentage of images with $Y = 1$ label constant at 8.5%. This means that we only add black boxes to 8.5% of training examples with the label $Y = 1$. We generate four datasets with different proportions of examples with the $Y = 0$ label altered: 30%, 50%, 70% and 90%. This results in spurious correlations between the added black box and the $Y = 0$ label.\n",
    "\n",
    "We consider the task of classifying digits 1 and 7. And we alter the data such that the digit 1 is spuriously correlated with the added black box. Concretely, inputs are `mnist` digits, labels are digit number Y = {1, 7}, and suprious attributes are black boxes, A = {present, absent}, with added black boxes associated with digit 7. Of the four groups, digit 1 with black boxes are the smallest group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IRsgDeRzJ76z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WMP/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rx92mxAzJ760"
   },
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yflBKwzpJ760"
   },
   "outputs": [],
   "source": [
    "# Pick out two classes of digits: 1, 7 and take a subset of samples \n",
    "X_subset = X[((y == '1') | (y == '7'))]\n",
    "y_subset = y[((y == '1') | (y == '7'))]\n",
    "\n",
    "# Encode the label '1' with y = 1, and the label '7' with y = 0\n",
    "y_subset[y_subset == '1'] = 1\n",
    "y_subset[y_subset == '7'] = 0\n",
    "y_subset = y_subset.astype(int)\n",
    "\n",
    "# Split into training and testing sets\n",
    "# Following celebA: 80% training, 10% val, 10% test \n",
    "rand_seed = 123\n",
    "X_train, X_test_tot, y_train, y_test_tot = train_test_split(X_subset, y_subset, test_size = 0.2, random_state = rand_seed) # split out train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_tot, y_test_tot, test_size = 0.5, random_state = rand_seed) # split out validation set\n",
    "\n",
    "# save y_train, y_val, y_test labels in data folder\n",
    "np.savetxt('data/y_train.csv', y_train, delimiter = ',')\n",
    "np.savetxt('data/y_val.csv', y_val, delimiter = ',')\n",
    "np.savetxt('data/y_test.csv', y_test, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "w3QorZyvJ762",
    "outputId": "304edb89-110a-47aa-df8a-32459834a1f3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some samples from the training dataset \n",
    "fig, ax = plt.subplots(2, 10, figsize = (25, 4))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(X_train[y_train == 0][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
    "    ax[1, i].imshow(X_train[y_train == 1][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows some example (unaltered) images from the `mnist` dataset. Specifically, our task is to classify digit 1 vs. 7. The following function alters specific proportions of these images in order to introduce spurious features (i.e., a black box on the upper left-hand corner of an image that appears more often on digit 7 as compared to digit 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WOrfI5UGJ764"
   },
   "outputs": [],
   "source": [
    "# function to add a grey rectangular box to top left-hand corner of digit\n",
    "\n",
    "def add_spurious_ft(x_array, y_array, perc_spurious_1, perc_spurious_2):\n",
    "    '''\n",
    "    this function takes in x and y arrays, adds spurious features to some images, and returns \n",
    "    the altered image array, along with indices of the images that were altered. spurious features\n",
    "    include a grey box on the upper lefthand side of the image.\n",
    "    \n",
    "    x_array: array of shape (n_images, 784)\n",
    "    y_array: array of shape (n_images, )\n",
    "    perc_spurious_1: percent of digit 1 images to add spurious features to\n",
    "    perc_spurious_2: percent of digit 1 images to add spurious features to\n",
    "\n",
    "    returns: \n",
    "        altered image array\n",
    "        indices of altered images (0 = not altered, 1 = altered)\n",
    "    '''\n",
    "    \n",
    "    indices_1 = np.asarray(np.where(y_train == 1)).reshape(-1) # indices where digit = 1 \n",
    "    indices_2 = np.asarray(np.where(y_train == 0)).reshape(-1) # indices where digit = 2 \n",
    "    \n",
    "    num_digit_1 = len(indices_1) # number of digit 1 images \n",
    "    num_digit_2 = len(indices_2) # number of digit 2 images \n",
    "\n",
    "    num_alter_1 = int(perc_spurious_1 * num_digit_1) # number of digit 1 images to alter\n",
    "    num_alter_2 = int(perc_spurious_2 * num_digit_2) # number of digit 2 images to alter\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    rand_indices_1 = np.random.choice(len(indices_1), size = num_alter_1, replace = False) \n",
    "    rand_indices_2 = np.random.choice(len(indices_2), size = num_alter_2, replace = False) \n",
    "    \n",
    "    grey_boxes = np.r_[0:15, 28:43, 56:71]\n",
    "    altered_imgs = x_array\n",
    "    \n",
    "    image_altered = []\n",
    "    \n",
    "    for i in range(len(altered_imgs)):\n",
    "        \n",
    "        # alter digit 7 images \n",
    "        if y_train[i] == 0:\n",
    "            if i in rand_indices_2:\n",
    "                altered_imgs[i][grey_boxes] = np.array([256] * len(grey_boxes))\n",
    "                image_altered.append(1) # 1 = altered \n",
    "            else:\n",
    "                image_altered.append(0) # 0 = not altered\n",
    "        \n",
    "        # alter digit 1 images \n",
    "        elif y_train[i] == 1:\n",
    "            if i in rand_indices_1:\n",
    "                altered_imgs[i][grey_boxes] = np.array([256] * len(grey_boxes))\n",
    "                image_altered.append(1)\n",
    "            else: \n",
    "                image_altered.append(0)\n",
    "    \n",
    "    return altered_imgs, np.array(image_altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BXqXhZHAJ764"
   },
   "outputs": [],
   "source": [
    "# test to see if the function works \n",
    "test_altered_imgs, test_altered_indices = add_spurious_ft(X_train, y_train, 0.085, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "phyaUoBfJ765",
    "outputId": "536fd8b2-c394-4c32-f13c-9968f89f8cd7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some samples from the altered training dataset. more 7's with spurious features than 1's. seems to work! \n",
    "fig, ax = plt.subplots(2, 10, figsize = (25, 4))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(test_altered_imgs[y_train == 0][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
    "    ax[1, i].imshow(test_altered_imgs[y_train == 1][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are now altered to include a spurious feature, such that the presence of the black box is \"spuriously correlated\" with digit 7 (just as female gender was spuriously correlated with blond hair in the `CelebA` dataset used in the paper by Jones et al., 2021). \n",
    "\n",
    "To explore the impact of the strength of this spurious correlation, we now use the above function to generate 4 sets of image datasets (each with a training, validation, and test set) with different percent of spurious features present in digit 7 (while holding the percept of spurious features in digit 1 constant at 8.5; following the `CelebA` dataset).\n",
    "\n",
    "`X_train_9`, `X_val_9`, `X_test_9`: 90% spurious features in digit 7; 8.5% spurious features in digit 1.\n",
    "\n",
    "`X_train_7`, `X_val_7`, `X_test_7`: 70% spurious features in digit 7; 8.5% spurious features in digit 1.\n",
    "\n",
    "`X_train_5`, `X_val_5`, `X_test_5`: 50% spurious features in digit 7; 8.5% spurious features in digit 1.\n",
    "\n",
    "`X_train_3`, `X_val_3`, `X_test_3`: 30% spurious features in digit 7; 8.5% spurious features in digit 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kHnBa-BUJ766"
   },
   "outputs": [],
   "source": [
    "pct_label_0 = 0.085\n",
    "# 90% spurious in digit 7 \n",
    "X_train_9, X_train_9_altindices = add_spurious_ft(X_train, y_train, pct_label_0, 0.9)\n",
    "X_val_9, X_val_9_altindices = add_spurious_ft(X_val, y_val, pct_label_0, 0.9)\n",
    "X_test_9, X_test_9_altindices = add_spurious_ft(X_test, y_test, pct_label_0, 0.9)\n",
    "\n",
    "# 70% spurious in digit 7 \n",
    "X_train_7, X_train_7_altindices = add_spurious_ft(X_train, y_train, pct_label_0, 0.7)\n",
    "X_val_7, X_val_7_altindices = add_spurious_ft(X_val, y_val, pct_label_0, 0.7)\n",
    "X_test_7, X_test_7_altindices = add_spurious_ft(X_test, y_test, pct_label_0, 0.7)\n",
    "\n",
    "# 50% spurious in digit 7\n",
    "X_train_5, X_train_5_altindices = add_spurious_ft(X_train, y_train, pct_label_0, 0.5)\n",
    "X_val_5, X_val_5_altindices = add_spurious_ft(X_val, y_val, pct_label_0, 0.5)\n",
    "X_test_5, X_test_5_altindices = add_spurious_ft(X_test, y_test, pct_label_0, 0.5)\n",
    "\n",
    "# 30% spurious in digit7\n",
    "X_train_3, X_train_3_altindices = add_spurious_ft(X_train, y_train, pct_label_0, 0.3)\n",
    "X_val_3, X_val_3_altindices = add_spurious_ft(X_val, y_val, pct_label_0, 0.3)\n",
    "X_test_3, X_test_3_altindices = add_spurious_ft(X_test, y_test, pct_label_0, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FC4FV6fNJ767"
   },
   "outputs": [],
   "source": [
    "# save altered indices labels \n",
    "\n",
    "np.savetxt('data/altered_index_train_9.csv', X_train_9_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_val_9.csv', X_val_9_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_test_9.csv', X_test_9_altindices, delimiter = ',')\n",
    "\n",
    "np.savetxt('data/altered_index_train_7.csv', X_train_7_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_val_7.csv', X_val_7_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_test_7.csv', X_test_7_altindices, delimiter = ',')\n",
    "\n",
    "np.savetxt('data/altered_index_train_5.csv', X_train_5_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_val_5.csv', X_val_5_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_test_5.csv', X_test_5_altindices, delimiter = ',')\n",
    "\n",
    "np.savetxt('data/altered_index_train_3.csv', X_train_3_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_val_3.csv', X_val_3_altindices, delimiter = ',')\n",
    "np.savetxt('data/altered_index_test_3.csv', X_test_3_altindices, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ygmt6RKWki"
   },
   "source": [
    "## 5b. Data Processing and Modeling\n",
    "\n",
    "We develop a neural network to classify the images in the four datasets produced. Designing the architecture of the model took time as we tried to replicate the trends of the paper. Once we train a model for each of the four datasets, we investigate the margin distributions and the accuracy coverage curve to compare our results with those in the paper.\n",
    "\n",
    "First, to prepare the synthetic data for a ResNet NN, we perform some data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-UqGzOKLcAL",
    "outputId": "55444cce-a129-4a4f-c140-157e4c4dbe37"
   },
   "outputs": [],
   "source": [
    "## Reshape the datasets\n",
    "# 90% spurious in digit 2 reshaped\n",
    "X_train_9_reshaped = X_train_9.reshape((X_train_9.shape[0], 28, 28))\n",
    "X_val_9_reshaped = X_val_9.reshape((X_val_9.shape[0], 28, 28))\n",
    "X_test_9_reshaped = X_test_9.reshape((X_test_9.shape[0], 28, 28))\n",
    "\n",
    "# # 70% spurious in digit 2 reshaped\n",
    "X_train_7_reshaped = X_train_7.reshape((X_train_7.shape[0], 28, 28))\n",
    "X_val_7_reshaped = X_val_7.reshape((X_val_7.shape[0], 28, 28))\n",
    "X_test_7_reshaped = X_test_7.reshape((X_test_7.shape[0], 28, 28))\n",
    "\n",
    "\n",
    "# # 50% spurious in digit 2 reshaped\n",
    "X_train_5_reshaped = X_train_5.reshape((X_train_5.shape[0], 28, 28))\n",
    "X_val_5_reshaped = X_val_5.reshape((X_val_5.shape[0], 28, 28))\n",
    "X_test_5_reshaped = X_test_5.reshape((X_test_5.shape[0], 28, 28))\n",
    "\n",
    "\n",
    "# # 30% spurious in digit 2 reshaped\n",
    "X_train_3_reshaped = X_train_3.reshape((X_train_3.shape[0], 28, 28))\n",
    "X_val_3_reshaped = X_val_3.reshape((X_val_3.shape[0], 28, 28))\n",
    "X_test_3_reshaped = X_test_3.reshape((X_test_3.shape[0], 28, 28))\n",
    "\n",
    "# # # Exapand the datasets\n",
    "# expand new axis, channel axis \n",
    "x_train_x9 = np.expand_dims(X_train_9_reshaped, axis = -1)\n",
    "x_test_x9 = np.expand_dims(X_test_9_reshaped, axis = -1)\n",
    "x_val_x9 = np.expand_dims(X_val_9_reshaped, axis = -1)\n",
    "\n",
    "x_train_x7 = np.expand_dims(X_train_7_reshaped, axis = -1)\n",
    "x_test_x7 = np.expand_dims(X_test_7_reshaped, axis = -1)\n",
    "x_val_x7 = np.expand_dims(X_val_7_reshaped, axis = -1)\n",
    "\n",
    "x_train_x5 = np.expand_dims(X_train_5_reshaped, axis = -1)\n",
    "x_test_x5 = np.expand_dims(X_test_5_reshaped, axis = -1)\n",
    "x_val_x5 = np.expand_dims(X_val_5_reshaped, axis = -1)\n",
    "\n",
    "x_train_x3 = np.expand_dims(X_train_3_reshaped, axis = -1)\n",
    "x_test_x3 = np.expand_dims(X_test_3_reshaped, axis = -1)\n",
    "x_val_x3 = np.expand_dims(X_val_3_reshaped, axis = -1)\n",
    "\n",
    "# We need 3 channel (instead of 1)\n",
    "channel_num = 3\n",
    "x_train_x9 = np.repeat(x_train_x9, channel_num, axis = -1)\n",
    "x_test_x9 = np.repeat(x_test_x9, channel_num, axis = -1)\n",
    "x_val_x9 = np.repeat(x_val_x9, channel_num, axis = -1)\n",
    "\n",
    "x_train_x7 = np.repeat(x_train_x7, channel_num, axis = -1)\n",
    "x_test_x7 = np.repeat(x_test_x7, channel_num, axis = -1)\n",
    "x_val_x7 = np.repeat(x_val_x7, channel_num, axis = -1)\n",
    "\n",
    "x_train_x5 = np.repeat(x_train_x5, channel_num, axis = -1)\n",
    "x_test_x5 = np.repeat(x_test_x5, channel_num, axis = -1)\n",
    "x_val_x5 = np.repeat(x_val_x5, channel_num, axis = -1)\n",
    "\n",
    "x_train_x3 = np.repeat(x_train_x3, channel_num, axis = -1)\n",
    "x_test_x3 = np.repeat(x_test_x3, channel_num, axis = -1)\n",
    "x_val_x3 = np.repeat(x_val_x3, channel_num, axis = -1)\n",
    "\n",
    "# normalize \n",
    "normalization = 255\n",
    "x_train_x9 = x_train_x9.astype('float32') / normalization\n",
    "x_test_x9 = x_test_x9.astype('float32') / normalization\n",
    "x_val_x9 = x_val_x9.astype('float32') / normalization\n",
    "\n",
    "x_train_x7 = x_train_x7.astype('float32') / normalization\n",
    "x_test_x7 = x_test_x7.astype('float32') / normalization\n",
    "x_val_x7 = x_val_x7.astype('float32') / normalization\n",
    "\n",
    "x_train_x5 = x_train_x5.astype('float32') / normalization\n",
    "x_test_x5 = x_test_x5.astype('float32') / normalization\n",
    "x_val_x5 = x_val_x5.astype('float32') / normalization\n",
    "x_train_x3 = x_train_x3.astype('float32') / normalization\n",
    "x_test_x3 = x_test_x3.astype('float32') / normalization\n",
    "x_val_x3 = x_val_x3.astype('float32') / normalization\n",
    "\n",
    "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
    "input_shape = 32\n",
    "x_train_x9 = tf.image.resize(x_train_x9, [input_shape, input_shape]) \n",
    "x_test_x9 = tf.image.resize(x_test_x9, [input_shape, input_shape]) \n",
    "x_val_x9 = tf.image.resize(x_val_x9, [input_shape, input_shape])  \n",
    "\n",
    "x_train_x7 = tf.image.resize(x_train_x7, [input_shape, input_shape]) \n",
    "x_test_x7 = tf.image.resize(x_test_x7, [input_shape, input_shape]) \n",
    "x_val_x7 = tf.image.resize(x_val_x7, [input_shape, input_shape]) \n",
    "\n",
    "x_train_x5 = tf.image.resize(x_train_x5, [input_shape, input_shape]) \n",
    "x_test_x5 = tf.image.resize(x_test_x5, [input_shape, input_shape]) \n",
    "x_val_x5 = tf.image.resize(x_val_x5, [input_shape, input_shape])  \n",
    "\n",
    "x_train_x3 = tf.image.resize(x_train_x3, [input_shape, input_shape]) \n",
    "x_test_x3 = tf.image.resize(x_test_x3, [input_shape, input_shape])  \n",
    "x_val_x3 = tf.image.resize(x_val_x3, [input_shape, input_shape]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of epochs here. For testing, we can use 1?\n",
    "num_epochs = 10\n",
    "\n",
    "# To save time, we have saved the results of the following code in .csv files\n",
    "# If you want to retrain the models, set the following boolean to True\n",
    "train_all_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet_model(x_train, y_train, x_val, y_val, x_test):\n",
    "    ''' This function loads the ResNet with preset weights, adds three layers, and fits the model.\n",
    "    parameters\n",
    "    ------\n",
    "    \n",
    "    returns\n",
    "    ------\n",
    "    history , predict : fitted model, predictions on train\n",
    "    '''\n",
    "    inputs = tf.keras.Input(shape = (input_shape, input_shape, 3))\n",
    "    \n",
    "    # Load ResNet with preset weights from \"imagenet\" training\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        input_tensor = inputs,\n",
    "        include_top = False,\n",
    "        weights = 'imagenet'\n",
    "    )\n",
    "\n",
    "    base_model.traninable = False\n",
    "    \n",
    "    # add additional layers\n",
    "#         Add three layers at the end of ResNet:\n",
    "#         A global average 2d pooling layer\n",
    "#         A dropout layer\n",
    "#         And a trainable binary classifier equivalent to the one that predicts spurious feature\n",
    "    add_model = tf.keras.Sequential()\n",
    "    add_model.add(base_model)\n",
    "    add_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    add_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    add_model.add(tf.keras.layers.Dense(2, \n",
    "                        activation='softmax'))\n",
    "    \n",
    "    # instantiate model for prediction\n",
    "    model = add_model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = num_epochs)\n",
    "\n",
    "    predict = model.predict(x_test)\n",
    "    \n",
    "    return history, predict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ResNet model with 90% spurious correlation\n",
    "if (train_all_models == True):\n",
    "    history_9, predict_9 = train_resnet_model(x_train = x_train_x9,\n",
    "                                              y_train = y_train,\n",
    "                                              x_val = x_val_x9,\n",
    "                                              y_val = y_val, \n",
    "                                              x_test = x_test_x9)\n",
    "    # re-save the prediction\n",
    "    np.savetxt('data/RESNET_predict_9.csv', predict_9, delimiter = ',')\n",
    "else:\n",
    "    predict_9 = np.genfromtxt('data/RESNET_predict_9.csv', delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_all_models == True):\n",
    "    # train Resnet model with 70% spurious correlation\n",
    "    history_7, predict_7 = train_resnet_model(x_train = x_train_x7,\n",
    "                                              y_train = y_train,\n",
    "                                              x_val = x_val_x7,\n",
    "                                              y_val = y_val, \n",
    "                                              x_test = x_test_x7)\n",
    "    \n",
    "    np.savetxt('data/RESNET_predict_7.csv', predict_7, delimiter = ',')\n",
    "else:\n",
    "    predict_7 = np.genfromtxt('data/RESNET_predict_7.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_all_models == True):\n",
    "    # train Resnet model with 50% spurious correlation\n",
    "    history_5, predict_5 = train_resnet_model(x_train = x_train_x5,\n",
    "                                              y_train = y_train,\n",
    "                                              x_val = x_val_x5,\n",
    "                                              y_val = y_val, \n",
    "                                              x_test = x_test_x5)\n",
    "    \n",
    "    np.savetxt('data/RESNET_predict_5.csv', predict_5, delimiter = ',')\n",
    "else:\n",
    "    predict_5 = np.genfromtxt('data/RESNET_predict_5.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_all_models == True):\n",
    "    # train Resnet model with 30% spurious correlation\n",
    "    history_3, predict_3 = train_resnet_model(x_train = x_train_x3,\n",
    "                                              y_train = y_train,\n",
    "                                              x_val = x_val_x3,\n",
    "                                              y_val = y_val, \n",
    "                                              x_test = x_test_x3)\n",
    "    \n",
    "    np.savetxt('data/RESNET_predict_3.csv', predict_3, delimiter = ',')\n",
    "else:\n",
    "    predict_3 = np.genfromtxt('data/RESNET_predict_3.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnq0shSxZc8i"
   },
   "source": [
    "As a comparison to the state-of-the-art ResNet models trained above, we also use Keras to train a simple 2-layer neural network for each of the spurious datasets (90%, 70%, 50%, 30%). These models can help us explore the impact of simple vs. complex models with transfer learning on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4av5AUb0Znuw"
   },
   "outputs": [],
   "source": [
    "## Train keras model with 90% spurious correlation dataset\n",
    "reg_model_9 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (input_shape, input_shape,3)), \n",
    "                                    tf.keras.layers.Dense(128, activation = tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-e1DijNkZygv",
    "outputId": "eec2a59d-64c6-4b60-c060-252a03c1f5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 393,473\n",
      "Trainable params: 393,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_model_9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWcNKY6iandp",
    "outputId": "ee473537-2873-47dc-cd34-8af4f8a753cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "380/380 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9608 - val_loss: 0.0636 - val_accuracy: 0.9848\n",
      "Epoch 2/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9904 - val_loss: 0.0461 - val_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9924 - val_loss: 0.0362 - val_accuracy: 0.9901\n",
      "Epoch 4/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9929 - val_loss: 0.0294 - val_accuracy: 0.9914\n",
      "Epoch 5/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 0.0279 - val_accuracy: 0.9921\n",
      "Epoch 6/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9933 - val_loss: 0.0273 - val_accuracy: 0.9927\n",
      "Epoch 7/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.0243 - val_accuracy: 0.9927\n",
      "Epoch 8/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.0263 - val_accuracy: 0.9941\n",
      "Epoch 9/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.0242 - val_accuracy: 0.9941\n",
      "Epoch 10/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.0211 - val_accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "reg_model_9.compile(optimizer ='sgd',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model 9 regular 2 layer rnn compile and fit\n",
    "reg_model_9_history = reg_model_9.fit(x = x_train_x9,\n",
    "                                      y = y_train,\n",
    "                                      validation_data = (x_val_x9, y_val), \n",
    "                                      epochs = num_epochs)\n",
    "\n",
    "\n",
    "predict_nn_9 = reg_model_9.predict(x_test_x9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/NN_predict_9.csv', predict_nn_9, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vWrM_mdNZ0Mr"
   },
   "outputs": [],
   "source": [
    "## Train keras model with 70% spurious correlation dataset\n",
    "reg_model_7 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (input_shape, input_shape,3)), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L_2Ap9VaFwj",
    "outputId": "2699ca92-3774-4ac2-c730-e8879fbc4325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 393,473\n",
      "Trainable params: 393,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9uz0DTabPuB",
    "outputId": "4bbd642c-0ee2-45d8-c1f8-fcae87c105ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "380/380 [==============================] - 1s 1ms/step - loss: 0.1697 - accuracy: 0.9588 - val_loss: 0.0635 - val_accuracy: 0.9842\n",
      "Epoch 2/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9917 - val_loss: 0.0433 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 0.9922 - val_loss: 0.0355 - val_accuracy: 0.9901\n",
      "Epoch 4/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0249 - accuracy: 0.9934 - val_loss: 0.0303 - val_accuracy: 0.9901\n",
      "Epoch 5/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.0287 - val_accuracy: 0.9914\n",
      "Epoch 6/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 0.0276 - val_accuracy: 0.9914\n",
      "Epoch 7/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.0249 - val_accuracy: 0.9934\n",
      "Epoch 8/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9952 - val_loss: 0.0243 - val_accuracy: 0.9927\n",
      "Epoch 9/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 0.0217 - val_accuracy: 0.9927\n",
      "Epoch 10/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9940 - val_loss: 0.0236 - val_accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "reg_model_7.compile(optimizer ='sgd',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model 7 regular 2 layer rnn compile and fit\n",
    "reg_model_7_history = reg_model_7.fit(x=x_train_x7,y=y_train, validation_data=(x_val_x7, y_val), epochs=10)\n",
    "\n",
    "\n",
    "predict_nn_7 = reg_model_7.predict(x_test_x7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/NN_predict_7.csv', predict_nn_7, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lSIXZLH4Z2yZ"
   },
   "outputs": [],
   "source": [
    "## Train keras model with 50% spurious correlation dataset\n",
    "reg_model_5 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (input_shape, input_shape, 3)), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLh6HjixaG1e",
    "outputId": "c0e6ac0e-6570-4500-cdb6-cf6f49697a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 393,473\n",
      "Trainable params: 393,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZw7Chdjbo3T",
    "outputId": "9adbc2de-0757-477a-dd73-5b08b38e024d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "380/380 [==============================] - 1s 1ms/step - loss: 0.1731 - accuracy: 0.9589 - val_loss: 0.0580 - val_accuracy: 0.9835\n",
      "Epoch 2/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0354 - accuracy: 0.9899 - val_loss: 0.0460 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0314 - accuracy: 0.9915 - val_loss: 0.0405 - val_accuracy: 0.9895\n",
      "Epoch 4/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9927 - val_loss: 0.0304 - val_accuracy: 0.9908\n",
      "Epoch 5/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.0290 - val_accuracy: 0.9914\n",
      "Epoch 6/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.0258 - val_accuracy: 0.9927\n",
      "Epoch 7/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0244 - val_accuracy: 0.9927\n",
      "Epoch 8/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.0219 - val_accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 0.0214 - val_accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.0209 - val_accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "reg_model_5.compile(optimizer ='sgd',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model 5 regular 2 layer rnn compile and fit\n",
    "reg_model_5_history = reg_model_5.fit(x=x_train_x5,y=y_train, validation_data=(x_val_x5, y_val), epochs=10)\n",
    "\n",
    "\n",
    "predict_nn_5 = reg_model_5.predict(x_test_x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/NN_predict_5.csv', predict_nn_5, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nbeCvDdsZ-Bh"
   },
   "outputs": [],
   "source": [
    "## Train keras model with 30% spurious correlation dataset\n",
    "reg_model_3 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (input_shape, input_shape,3)), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GX2GBC1RaHz5",
    "outputId": "4bf41070-fca9-4f35-963f-fd04f2ba87ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 393,473\n",
      "Trainable params: 393,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6P8IwkNhbqTJ",
    "outputId": "bd4c6558-a616-4517-db21-f344e05199e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "380/380 [==============================] - 1s 1ms/step - loss: 0.1700 - accuracy: 0.9591 - val_loss: 0.0649 - val_accuracy: 0.9842\n",
      "Epoch 2/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 0.9917 - val_loss: 0.0463 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0299 - accuracy: 0.9921 - val_loss: 0.0373 - val_accuracy: 0.9895\n",
      "Epoch 4/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0254 - accuracy: 0.9935 - val_loss: 0.0342 - val_accuracy: 0.9901\n",
      "Epoch 5/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9924 - val_loss: 0.0271 - val_accuracy: 0.9908\n",
      "Epoch 6/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9940 - val_loss: 0.0272 - val_accuracy: 0.9914\n",
      "Epoch 7/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.0251 - val_accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9947 - val_loss: 0.0231 - val_accuracy: 0.9921\n",
      "Epoch 9/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.0220 - val_accuracy: 0.9927\n",
      "Epoch 10/10\n",
      "380/380 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0216 - val_accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "reg_model_3.compile(optimizer ='sgd',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model 3 regular 2 layer rnn compile and fit\n",
    "reg_model_3_history = reg_model_3.fit(x=x_train_x3,y=y_train, validation_data=(x_val_x3, y_val), epochs=10)\n",
    "\n",
    "\n",
    "predict_nn_3 = reg_model_3.predict(x_test_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/NN_predict_3.csv', predict_nn_3, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Our Results - Replication and Extension\n",
    "#### What did we find, and how do our results align with the paper (Jones et al., 2021)?\n",
    "\n",
    "First, we plot the margin distribution, which captures the model’s confidences across all predictions and\n",
    "determines which examples it abstains on at each threshold. Formally, a selective classifier that makes prediction, $\\hat{y}$ on a point $x$ with a confidence $\\hat{c}$ is evaluated based on some confidence threshold $\\tau$. The margin distribution is a plot of the confidence margin on the $x$ axis and the density of points on the $y$ axis. The region of the plot that falls between $-\\tau$ and $+\\tau$ are points where the classifier abstains. We plot the overall margin distribution as well as the margin distribution of the worst group.\n",
    "\n",
    "When we vary this confidence threshold, $\\tau$, we can analyze the accuracy coverage where coverage is defined as the proportion of predicted points. This means that full coverage (100%) corresponds to a classifier that does not abstain on any predictions. Selective accuracy is defined as the average accuracy over all points and the worst group accuracy is the accuracy of points that are part of the worst performing group. The paper explores whether selective classification helps or hurts accuracy and how overall accuracy compares to the accuracy of different subgroups.\n",
    "\n",
    "The most striking finding of the paper shows that worst-group accuracy decreases with coverage. As in, the more confident the model is on the worst-group examples, the more incorrect it is. Let us examine to what degree we can recreate this result with our synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # set seaborn to make plots look nice\n",
    "# variable for margin range\n",
    "tau_max = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(logit, actual_class):\n",
    "    \"\"\" Calculate the predictive confidence according to Eq 1 (Jones et al.)\n",
    "    \n",
    "        Parameters:\n",
    "            logit (tuple or list of tuples) : The probability of each prediction (P(Y = 1), P(Y = 0))\n",
    "            actual_class (int or list of ints) : Correct class labels (0 or 1) of the predictions\n",
    "        Returns :\n",
    "            c_hat (float or list of floats) : The confidence of each prediction\n",
    "                = 0.5 * log(max(logit_i) / (1 - max(logit_i))) if argmax(logit_i) == predicted_class_i\n",
    "                else\n",
    "                = -0.5 * log(max(logit_i) / (1 - max(logit_i))) if argmax(logit_i) != predicted_class_i\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_class = np.argmax(logit, axis = 1)\n",
    "    c_hat = [0.5 * np.log10(l[pc] / (1 - l[pc])) for l, pc in zip(logit, predicted_class)]\n",
    "    c_hat = [c if c != np.inf else (tau_max - 1) for c in c_hat] #if the confidence is infinity (i.e. P(Y = 1) or P(Y = 0) = 1), set confidence to arbitrary large number\n",
    "    c_hat = [c if pc == ac else -c for c, pc, ac in zip(c_hat, predicted_class, actual_class)]\n",
    "    return(c_hat)\n",
    "\n",
    "def dropout(confidence, tau):\n",
    "    \"\"\" Determine predictions above a threshold\n",
    "    \n",
    "        Parameters:\n",
    "            confidence (float or list of floats) : predictive confidence according to Eq 1 (Jones et al)\n",
    "            tau (float) : threshold for abstaining on predictions\n",
    "        Returns:\n",
    "            (float or list of floats) : the predictive confidence that exceeds (in absolute value terms) the threshold \n",
    "    \"\"\"\n",
    "    return(confidence[np.abs(confidence) > tau])\n",
    "\n",
    "def coverage(confidence, tau):\n",
    "    \"\"\" Determine the number of samples that are predicted above a confidence threshold\n",
    "    \n",
    "        Parameters:\n",
    "            confidence (float or list of floats) : predictive confidence according to Eq 1 (Jones et al)\n",
    "            tau (float) : threshold for abstaining on predictions\n",
    "        Returns:\n",
    "            (float) : the fraction of samples between 0 and 1 (inclusive) that are predicted with confidence above threshold tau\n",
    "    \"\"\"\n",
    "    dropped_out = dropout(confidence, tau)\n",
    "    return(len(dropped_out) / len(confidence))\n",
    "\n",
    "def accuracy(confidence, tau):\n",
    "    \"\"\" Determine the fraction of correctly predicted labels above a confidence threshold\n",
    "    \n",
    "    Parameters:\n",
    "            confidence (float or list of floats) : predictive confidence according to Eq 1 (Jones et al)\n",
    "            tau (float) : threshold for abstaining on predictions\n",
    "    Returns:\n",
    "            (float) : the fraction of samples between 0 and 1 (inclusive) that are correctly predicted \n",
    "        \n",
    "    \"\"\"\n",
    "    dropped_out = dropout(confidence, tau)\n",
    "    return(np.sum(dropped_out > 0) / len(dropped_out))\n",
    "\n",
    "def plot_margin(confidence, worst_group_idx, spur_proportion, model_type = 'RESNET'):\n",
    "    ''' This function plots the confidence margin on the x axis \n",
    "    and the density of points on the y axis to compare the full\n",
    "    dataset performance with the worst group performance.\n",
    "    '''\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8, 5))\n",
    "    ax.hist(confidence, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax.hist(np.array(confidence)[worst_group_idx], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax.set_title(\"{} - Synthetic dataset: spurious correlations {}%\".format(model_type, spur_proportion), fontsize = 18)\n",
    "    ax.set_xlabel('Margin', fontsize = 14)\n",
    "    ax.set_ylabel('Ave. Density', fontsize = 14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_coverage(full_coverage, full_accuracy, worst_coverage, worst_accuracy, spur_proportion, model_type = 'RESNET'):\n",
    "    '''This function plots the coverage on the x axis and the accuracy on the y axis\n",
    "    for both the full dataset and the worst group in the dataset as defined in the paper.\n",
    "    '''\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(full_coverage, full_accuracy, label = 'All data', color = 'blue')\n",
    "    ax.plot(worst_coverage, worst_accuracy, label = 'Worst group', color = 'red')\n",
    "    ax.set_xlabel('Coverage', fontsize = 14)\n",
    "    ax.set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax.set_title(\"{} - Synthetic dataset: spurious correlations {}%\".format(model_type, spur_proportion), fontsize = 18)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. Results for ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "    # Plot margin subplots for RESNET\n",
    "    model_type = 'ResNet'\n",
    "    # 90% group\n",
    "    resnet_9_logits = predict_9\n",
    "    altered_labels_9 = X_test_9_altindices\n",
    "    confidence_resnet_9 = confidence(resnet_9_logits, y_test)\n",
    "    worst_group_9 = (y_test == 1.0) & (altered_labels_9 == 1.0) \n",
    "\n",
    "    # 70% group\n",
    "    resnet_7_logits = predict_7\n",
    "    altered_labels_7 = X_test_7_altindices\n",
    "    confidence_resnet_7 = confidence(resnet_7_logits, y_test)\n",
    "    worst_group_7 = (y_test == 1.0) & (altered_labels_7 == 1.0)\n",
    "\n",
    "    # 50% group\n",
    "    resnet_5_logits = predict_5\n",
    "    altered_labels_5 = X_test_5_altindices\n",
    "    confidence_resnet_5 = confidence(resnet_5_logits, y_test)\n",
    "    worst_group_5 = (y_test == 1.0) & (altered_labels_5 == 1.0) \n",
    "\n",
    "    # 30% group\n",
    "    resnet_3_logits = predict_3\n",
    "    altered_labels_3 = X_test_3_altindices\n",
    "    confidence_resnet_3 = confidence(resnet_3_logits, y_test)\n",
    "    worst_group_3 = (y_test == 1.0) & (altered_labels_3 == 1.0)\n",
    "\n",
    "    plot_y_max = 600\n",
    "    ave_line_width = 2\n",
    "\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (14, 9))\n",
    "    fig.tight_layout(pad = 5)\n",
    "    \n",
    "    ax[0 , 0].hist(confidence_resnet_9, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[0 , 0].hist(np.array(confidence_resnet_9)[worst_group_9], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[0 , 0].set_title(\"Spurious correlations {}%\".format(90), fontsize = 14)\n",
    "    ax[0 , 0].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[0 , 0].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[0 , 0].set_ylim([0, plot_y_max])\n",
    "    ax[0 , 0].axvline(np.mean(confidence_resnet_9), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 0].axvline(np.mean(np.array(confidence_resnet_9)[worst_group_9]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 0].legend()\n",
    "\n",
    "    ax[0 , 1].hist(confidence_resnet_7, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[0 , 1].hist(np.array(confidence_resnet_7)[worst_group_7], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[0 , 1].set_title(\"Spurious correlations {}%\".format(70), fontsize = 14)\n",
    "    ax[0 , 1].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[0 , 1].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[0 , 1].set_ylim([0, plot_y_max])\n",
    "    ax[0 , 1].axvline(np.mean(confidence_resnet_7), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 1].axvline(np.mean(np.array(confidence_resnet_7)[worst_group_7]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 1].legend()\n",
    "\n",
    "    ax[1, 0].hist(confidence_resnet_5, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[1, 0].hist(np.array(confidence_resnet_5)[worst_group_5], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[1, 0].set_title(\"Spurious correlations {}%\".format(50), fontsize = 14)\n",
    "    ax[1, 0].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[1, 0].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[1, 0].set_ylim([0, plot_y_max])\n",
    "    ax[1, 0].axvline(np.mean(confidence_resnet_5), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 0].axvline(np.mean(np.array(confidence_resnet_5)[worst_group_5]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 0].legend()\n",
    "\n",
    "    ax[1, 1].hist(confidence_resnet_3, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[1, 1].hist(np.array(confidence_resnet_3)[worst_group_3], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[1, 1].set_title(\"Spurious correlations {}%\".format(30), fontsize = 14)\n",
    "    ax[1, 1].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[1, 1].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[1, 1].set_ylim([0, plot_y_max])\n",
    "    ax[1, 1].axvline(np.mean(confidence_resnet_3), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 1].axvline(np.mean(np.array(confidence_resnet_3)[worst_group_3]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 1].legend()\n",
    "\n",
    "    fig.suptitle('{} - Ave Density for Full Dataset and Worst Group'.format(model_type), fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots present somewhat conflicting results. For the datasets with spurious correlations in 90%, 50%, and 30% of the digit 7 (i.e., overrepresented) data, we see that the worst group has worse performance than the full dataset on average. This aligns somewhat with the findings of Jones et al. (2021), although these differences are not as stark as the ones found in the paper. However, for the datasets with spurious correlations in 70% of the data, the worst group seems to do slightly *better* on average. \n",
    "\n",
    "**NOTE**: In some cases the confidence was $\\inf$. For purposes of visualization, we set these values to be arbitrarily high ($\\hat{c} = 5$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (14, 9))\n",
    "    fig.tight_layout(pad = 5)\n",
    "\n",
    "    tau = np.linspace(0, tau_max, 100)\n",
    "    full_coverage_9 = [coverage(np.array(confidence_resnet_9), t) for t in tau]\n",
    "    full_accuracy_9 = [accuracy(np.array(confidence_resnet_9), t) for t in tau]\n",
    "    worst_coverage_9 = [coverage(np.array(confidence_resnet_9)[worst_group_9], t) for t in tau]\n",
    "    worst_accuracy_9 = [accuracy(np.array(confidence_resnet_9)[worst_group_9], t) for t in tau]\n",
    "    \n",
    "    full_coverage_7 = [coverage(np.array(confidence_resnet_7), t) for t in tau]\n",
    "    full_accuracy_7 = [accuracy(np.array(confidence_resnet_7), t) for t in tau]\n",
    "    worst_coverage_7 = [coverage(np.array(confidence_resnet_7)[worst_group_7], t) for t in tau]\n",
    "    worst_accuracy_7 = [accuracy(np.array(confidence_resnet_7)[worst_group_7], t) for t in tau]\n",
    "    \n",
    "    full_coverage_5 = [coverage(np.array(confidence_resnet_5), t) for t in tau]\n",
    "    full_accuracy_5 = [accuracy(np.array(confidence_resnet_5), t) for t in tau]\n",
    "    worst_coverage_5 = [coverage(np.array(confidence_resnet_5)[worst_group_5], t) for t in tau]\n",
    "    worst_accuracy_5 = [accuracy(np.array(confidence_resnet_5)[worst_group_5], t) for t in tau]\n",
    "\n",
    "    full_coverage_3 = [coverage(np.array(confidence_resnet_3), t) for t in tau]\n",
    "    full_accuracy_3 = [accuracy(np.array(confidence_resnet_3), t) for t in tau]\n",
    "    worst_coverage_3 = [coverage(np.array(confidence_resnet_3)[worst_group_3], t) for t in tau]\n",
    "    worst_accuracy_3 = [accuracy(np.array(confidence_resnet_3)[worst_group_3], t) for t in tau]\n",
    "\n",
    "    ax[0,0].plot(full_coverage_9, full_accuracy_9, label = 'All data', color = 'blue')\n",
    "    ax[0,0].plot(worst_coverage_9, worst_accuracy_9, label = 'Worst group', color = 'red')\n",
    "    ax[0,0].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[0,0].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[0,0].set_title(\"Spurious correlations {}%\".format(90), fontsize = 14)\n",
    "    ax[0,0].legend()\n",
    "    \n",
    "    ax[0,1].plot(full_coverage_7, full_accuracy_7, label = 'All data', color = 'blue')\n",
    "    ax[0,1].plot(worst_coverage_7, worst_accuracy_7, label = 'Worst group', color = 'red')\n",
    "    ax[0,1].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[0,1].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[0,1].set_title(\"Spurious correlations {}%\".format(70), fontsize = 14)\n",
    "    ax[0,1].legend()\n",
    "    \n",
    "    ax[1,0].plot(full_coverage_5, full_accuracy_5, label = 'All data', color = 'blue')\n",
    "    ax[1,0].plot(worst_coverage_5, worst_accuracy_5, label = 'Worst group', color = 'red')\n",
    "    ax[1,0].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[1,0].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[1,0].set_title(\"Spurious correlations {}%\".format(50), fontsize = 14)\n",
    "    ax[1,0].legend()\n",
    "    \n",
    "    ax[1,1].plot(full_coverage_3, full_accuracy_3, label = 'All data', color = 'blue')\n",
    "    ax[1,1].plot(worst_coverage_3, worst_accuracy_3, label = 'Worst group', color = 'red')\n",
    "    ax[1,1].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[1,1].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[1,1].set_title(\"Spurious correlations {}%\".format(30), fontsize = 14)\n",
    "    ax[1,1].legend()\n",
    "    \n",
    "    fig.suptitle('{} - Accuracy-Coverage Curves for All Data and Worst Group'.format(model_type), fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots provide similar information to the margin distributions above, such that there does not seem to be a consistent, meaningful difference between the full data and the worst group regarding the relationship between coverage and accuracy. While the findings from Jones and colleagues (2021) would suggest that accuracy for the worst group should decrease at lower coverage, we see that this is not the case - but rather that the worst group accuracy is very high (almost at ceiling) for low coverage. This seems to suggest that model training was *too* good such that it potentially was not affected by the spurious features added. Therefore, we investigate findings when using a simpler Keras model below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Results for Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "    # Plot margin subplots for RESNET\n",
    "    model_type = 'Keras'\n",
    "    # 90% group\n",
    "    keras_9_logits =  np.hstack((1.0 - predict_nn_9, predict_nn_9))\n",
    "    altered_labels_9 = X_test_9_altindices\n",
    "    confidence_keras_9 = confidence(keras_9_logits, y_test)\n",
    "    \n",
    "\n",
    "    # 70% group\n",
    "    keras_7_logits =  np.hstack((1.0 - predict_nn_7, predict_nn_7) )\n",
    "    altered_labels_7 = X_test_7_altindices\n",
    "    confidence_keras_7 = confidence(keras_7_logits, y_test)\n",
    "\n",
    "    # 50% group\n",
    "    keras_5_logits =  np.hstack((1.0 - predict_nn_5, predict_nn_5) )\n",
    "    altered_labels_5 = X_test_5_altindices\n",
    "    confidence_keras_5 = confidence(keras_5_logits, y_test)\n",
    "\n",
    "    # 30% group\n",
    "    keras_3_logits =  np.hstack((1.0 - predict_nn_3, predict_nn_3) )\n",
    "    altered_labels_3 = X_test_3_altindices\n",
    "    confidence_keras_3 = confidence(keras_3_logits, y_test)\n",
    "\n",
    "    plot_y_max = 200\n",
    "    ave_line_width = 2\n",
    "\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (14, 9))\n",
    "    fig.tight_layout(pad = 5)\n",
    "    ax[0 , 0].hist(confidence_keras_9, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[0 , 0].hist(np.array(confidence_keras_9)[worst_group_9], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[0 , 0].set_title(\"Spurious correlations {}%\".format(90), fontsize = 14)\n",
    "    ax[0 , 0].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[0 , 0].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[0 , 0].set_ylim([0, plot_y_max])\n",
    "    ax[0 , 0].axvline(np.mean(confidence_keras_9), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 0].axvline(np.mean(np.array(confidence_keras_9)[worst_group_9]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 0].legend()\n",
    "\n",
    "    ax[0 , 1].hist(confidence_keras_7, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[0 , 1].hist(np.array(confidence_keras_7)[worst_group_7], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[0 , 1].set_title(\"Spurious correlations {}%\".format(70), fontsize = 14)\n",
    "    ax[0 , 1].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[0 , 1].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[0 , 1].set_ylim([0, plot_y_max])\n",
    "    ax[0 , 1].axvline(np.mean(confidence_keras_7), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 1].axvline(np.mean(np.array(confidence_keras_7)[worst_group_7]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[0 , 1].legend()\n",
    "\n",
    "    ax[1, 0].hist(confidence_keras_5, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[1, 0].hist(np.array(confidence_keras_5)[worst_group_5], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[1, 0].set_title(\"Spurious correlations {}%\".format(50), fontsize = 14)\n",
    "    ax[1, 0].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[1, 0].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[1, 0].set_ylim([0, plot_y_max])\n",
    "    ax[1, 0].axvline(np.mean(confidence_keras_5), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 0].axvline(np.mean(np.array(confidence_keras_5)[worst_group_5]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 0].legend()\n",
    "\n",
    "    ax[1, 1].hist(confidence_keras_3, label = 'full dataset', bins = 30, alpha = 0.5, color = 'blue')\n",
    "    ax[1, 1].hist(np.array(confidence_keras_3)[worst_group_3], label = 'worst group', bins = 30, alpha = 0.8, color = 'red')\n",
    "    ax[1, 1].set_title(\"Spurious correlations {}%\".format(30), fontsize = 14)\n",
    "    ax[1, 1].set_xlabel('Margin', fontsize = 14)\n",
    "    ax[1, 1].set_ylabel('Ave. Density', fontsize = 14)\n",
    "    ax[1, 1].set_ylim([0, plot_y_max])\n",
    "    ax[1, 1].axvline(np.mean(confidence_keras_3), color='b', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 1].axvline(np.mean(np.array(confidence_keras_3)[worst_group_3]), color='r', linestyle='dashed', linewidth=ave_line_width)\n",
    "    ax[1, 1].legend()\n",
    "\n",
    "    fig.suptitle('{} - Ave Density for Full Dataset and Worst Group'.format(model_type), fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots present results that are more in line with the findings from Jones et al. (2021), such that the worst group has consistently worse performance than the average group. Of note, however, these models do not seem to be making many incorrect predictions - rather, the worst group predictions are simply less confident than the full dataset. It seems that the accuracy is close to ceiling for these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (14, 9))\n",
    "    fig.tight_layout(pad = 5)\n",
    "\n",
    "    tau = np.linspace(0, tau_max, 100)\n",
    "    full_coverage_9 = [coverage(np.array(confidence_keras_9), t) for t in tau]\n",
    "    full_accuracy_9 = [accuracy(np.array(confidence_keras_9), t) for t in tau]\n",
    "    worst_coverage_9 = [coverage(np.array(confidence_keras_9)[worst_group_9], t) for t in tau]\n",
    "    worst_accuracy_9 = [accuracy(np.array(confidence_keras_9)[worst_group_9], t) for t in tau]\n",
    "    \n",
    "    full_coverage_7 = [coverage(np.array(confidence_keras_7), t) for t in tau]\n",
    "    full_accuracy_7 = [accuracy(np.array(confidence_keras_7), t) for t in tau]\n",
    "    worst_coverage_7 = [coverage(np.array(confidence_keras_7)[worst_group_7], t) for t in tau]\n",
    "    worst_accuracy_7 = [accuracy(np.array(confidence_keras_7)[worst_group_7], t) for t in tau]\n",
    "    \n",
    "    full_coverage_5 = [coverage(np.array(confidence_keras_5), t) for t in tau]\n",
    "    full_accuracy_5 = [accuracy(np.array(confidence_keras_5), t) for t in tau]\n",
    "    worst_coverage_5 = [coverage(np.array(confidence_keras_5)[worst_group_5], t) for t in tau]\n",
    "    worst_accuracy_5 = [accuracy(np.array(confidence_keras_5)[worst_group_5], t) for t in tau]\n",
    "\n",
    "    full_coverage_3 = [coverage(np.array(confidence_keras_3), t) for t in tau]\n",
    "    full_accuracy_3 = [accuracy(np.array(confidence_keras_3), t) for t in tau]\n",
    "    worst_coverage_3 = [coverage(np.array(confidence_keras_3)[worst_group_3], t) for t in tau]\n",
    "    worst_accuracy_3 = [accuracy(np.array(confidence_keras_3)[worst_group_3], t) for t in tau]\n",
    "\n",
    "    ax[0,0].plot(full_coverage_9, full_accuracy_9, label = 'All data', color = 'blue')\n",
    "    ax[0,0].plot(worst_coverage_9, worst_accuracy_9, label = 'Worst group', color = 'red')\n",
    "    ax[0,0].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[0,0].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[0,0].set_title(\"Spurious correlations {}%\".format(90), fontsize = 14)\n",
    "    ax[0,0].legend()\n",
    "    \n",
    "    ax[0,1].plot(full_coverage_7, full_accuracy_7, label = 'All data', color = 'blue')\n",
    "    ax[0,1].plot(worst_coverage_7, worst_accuracy_7, label = 'Worst group', color = 'red')\n",
    "    ax[0,1].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[0,1].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[0,1].set_title(\"Spurious correlations {}%\".format(70), fontsize = 14)\n",
    "    ax[0,1].legend()\n",
    "    \n",
    "    ax[1,0].plot(full_coverage_5, full_accuracy_5, label = 'All data', color = 'blue')\n",
    "    ax[1,0].plot(worst_coverage_5, worst_accuracy_5, label = 'Worst group', color = 'red')\n",
    "    ax[1,0].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[1,0].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[1,0].set_title(\"Spurious correlations {}%\".format(50), fontsize = 14)\n",
    "    ax[1,0].legend()\n",
    "    \n",
    "    ax[1,1].plot(full_coverage_3, full_accuracy_3, label = 'All data', color = 'blue')\n",
    "    ax[1,1].plot(worst_coverage_3, worst_accuracy_3, label = 'Worst group', color = 'red')\n",
    "    ax[1,1].set_xlabel('Coverage', fontsize = 14)\n",
    "    ax[1,1].set_ylabel('Accuracy', fontsize = 14)\n",
    "    ax[1,1].set_title(\"Spurious correlations {}%\".format(30), fontsize = 14)\n",
    "    ax[1,1].legend()\n",
    "    \n",
    "    fig.suptitle('Keras - Accuracy-Coverage Curves for All Data and Worst Group', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note on these plots is the y-axis, which does not even dip below 0.98. Therefore, our suspicion that these models are at ceiling seems to be accurate, such that regardless of coverage, the full dataset and worst group seem to be predicted nearly perfectly. It is possible that our synthetic data was not complex enough to lead to inaccurate classifications, even with this simple model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Conclusions and Final Evaluation\n",
    "\n",
    "In this paper, the authors claim that the results suggest that selective classification should be used with care and underscore the importance of training models to perform equally well across groups at full coverage. However, it seems that the results may rely on the dataset being particularly prone to spurious correlations and it is not clear that this phenomena is concerning across all domains that consider using selective classification. \n",
    "\n",
    "Consider our toy datasets and the addition of black boxes that are essentially meaningless in the context of the classification task. Whether or not an image contains an artificially added black box is not related to the number written in the image. In addition to developing better selective classification techniques that reduce group disparities at full coverage, it also seems natural to consider addressing problematic characteristics of the dataset. If there are in fact spurious correlations present in a given dataset, how meaningful are the correlations? In our toy example, we could simply remove the black boxes in order to avoid the contrasting result. This makes us wonder if there is a more naïve approach where practitioners better examine dataset characteristics before applying methodologies like selective classification in order to avoid these types of issues altogether.\n",
    "\n",
    "In our implementation, neither the ResNet nor the Keras models fully reproduced the problems identified by Jones and colleagues (2021). While the Keras models did seem to produce results that were more *consistent* with results from the paper, we were limited by a near-total absense of incorrect classifications. Of note, we purposely chose the digits 1 vs. 7 to increase \"difficulty\" of the problem (i.e., out of all the 10 digits, the digits 1 and 7 seem most similar to each other visually). However, this did not seem to cause problems with accurately classifying the digits, even with a simpler Keras model. \n",
    "\n",
    "In short, the experimental section of the paper could have explicitly dealt with dataset characteristics and responding to why this affect arises. We are still unclear if the results are the result of the model type of the specific datasets chosen in the paper. The paper could have benefited from stronger evaluation of the limitations of this affect.\n",
    "\n",
    "## 7b. Broader Impact\n",
    "\n",
    "Selective classification is meant to mitigate situations where the model makes inaccurate predictions on points. The entire point of selective classification is to offer the option of abstaining when the model is not confident and a prediction error is particularly costly. Consider high-stakes settings like criminal justice or medicine where inaccurate predictions result in real life human implications. In criminal justice, classification models that are used for example to predict whether a person should be eligible for bail based on how likely they are to re-offend need to be well formulated if used at all. If there are attributes correlated with the outcome (i.e. spurious correlations) and the selective classification ends up magnifying disparities in this context, then using the model to make decisions about someone's life is highly unethical. In medical settings where models might predict which patient receives treatment, magnifying disparate outcomes is particularly alarming. If selective classification is invoked to help mitigate these disparate outcomes, but is found to actually exasturbate them, these results are very important in terms of the broader societal impact.\n",
    "\n",
    "The stakeholders would depend on the setting. For criminal justice, the stakeholders include people up for bail decisions, judges and others involved in the decision making process. If the decision makers are unaware of the predictive confidence when using the model, they may wrongly associate too much confidence in the model. If the decision makers are made aware of selective classification and the option for the model to abstain, then this could result in the judge relying on domain and human knowledge to make the decision. There might also be individual circumstances that the model has not considered when making the decision. Consider a single mother who is up for bail and needs to be released in order to both care for her children but also make it to work in order to provide for her children. The judge deciding on her case should be very careful not to rely solely on the model's prediction, especially if it is inaccurate. Similarly, in the clinical setting, especially when it comes to mental health diagnosis, selective classification and abstaining on certain patients could be helpful to signal to the medical providers to take a second look at those patients. At the same time, overly confident predictions for certain patients could be damaging in its own way.\n",
    "\n",
    "When invoking selective classification as an option to abstain on certain predictions, there is a larger question around whether predictive models should be deployed at all in certain high-stakes settings with so much human impact. Even when trying to correct or mitigate the disparate impact of a model, it is important to come back to this question throughout the lifecycle of the model. As in, we should not take for granted the assumption that the model has to be used in any given setting in the first place.\n",
    "\n",
    "## 7c. Future work\n",
    "\n",
    "To build on our synthetic data analysis, future work should consider different types of synthetic datasets and multiclassification problems (we only consider the binary classification setting). Notice that the accuracy is very good for even the worst group in our example. Additional dataset types that may be more difficult to classify would be informative in terms of whether the magnification of disparate impacts is specific to settings where the accuracy is reasonable but not as good as ours.\n",
    "\n",
    "Finally, in the original paper, it is worth noting that none of the datasets seem particularly relevant to real-world application where bias and disparities are highly impactful (e.g., healthcare, predictive policing, criminal justice). The \"worst group\" is not underrepresented in the sense that individuals belonging to these groups experience social injustices, but rather in the sense that there are fewer examples of observations in this group (e.g., blond men being the \"worst group\" in `CelebA` dataset). While the framing of the paper encourages consideration of how selective classification can worsen group disparities, and this was demonstrated in various datasets, this finding would be much more compelling if it were applied in a dataset with meaningful real-world use cases. This would be an important future direction to inform building classification models in risk-adverse settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "gen_data_PT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
