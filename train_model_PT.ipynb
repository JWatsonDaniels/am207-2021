{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "deepnote_notebook_id": "000c1083-4610-4bab-bde7-2e0f6684bb45",
    "deepnote": {},
    "deepnote_execution_queue": [],
    "colab": {
      "name": "train_model_PT.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-6616920a-dc64-4134-b26d-168811364010",
        "deepnote_cell_type": "markdown",
        "id": "DtpLTfFFwnFt"
      },
      "source": [
        "## Selective Classification Can Magnify Disparities Across Groups\n",
        "\n",
        "\n",
        "### AM207: Advanced Scientific Computing\n",
        "\n",
        "#### Team: Jamelle Watson-Daniels, Shirley Wang, Bridger Ruyle, Paul Tembo\n",
        "\n",
        "#### Paper: https://arxiv.org/pdf/2010.14134.pdfODOS: \n",
        "\n",
        "Minimal goal: Investigate how selective classification affects group accuracy on a synthetic `mnist` dataset\n",
        "- Generate synthetic dataset from `mnist`\n",
        "- Train empirical risk minimization model (minimize loss - Logistic regression i.e. ResNet)    \n",
        "- Calculate average coverage\n",
        "- Calculate group coverage\n",
        "- Calculate selective accuracy\n",
        "- Plot average coverage vs group coverage for `mnist` dataset\n",
        "- Plot average coverage vs selective accuracy for `mnist` dataset\n",
        "\n",
        "If there is time goal: Show that selective accuracies on average and on the worst-group are determined by their respective margin distributions\n",
        "- Calculate margin\n",
        "- Calculate worst-group density\n",
        "- Calculate Average density\n",
        "- Plot margin (-10,+10) versus worst-group density\n",
        "- Plot margin (-10,+10) versus Average density\n",
        "\n",
        "Possible extensions: What are the characteristics of this dataset that are imbalanced and contain spurious correlations? We see that in all these datasets, the \"worst group\" is underrepresented as in there are much fewer examples for the \"worst group\" category. Notice that the datasets considered in this paper do not seem immediately relevant in terms of the real world implications of disparities, we wonder how we might examine this using toy examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-bbdd85dd-8bb0-4312-a402-70bd240aa52f",
        "deepnote_cell_type": "markdown",
        "id": "eHMAVaZtwnFu"
      },
      "source": [
        "#### Dataset description\n",
        "\"Models have been shown to latch onto spurious correlations between labels and demographic attributes such as race and gender (Buolamwini & Gebru, 2018; Joshi et al., 2018), and we\n",
        "study this on the CelebA dataset (Liu et al., 2015). Following Sagawa et al. (2020), we consider\n",
        "the task of classifying hair color, which is spuriously correlated with the gender. Concretely, inputs\n",
        "are celebrity face images, labels are hair color Y = {blond, non-blond}, and spurious attributes are\n",
        "gender, A = {male, female}, with blondness associated with being female. Of the four groups,\n",
        "blond males are the smallest group, with only 1,387 examples out of 162,770 training examples, and\n",
        "they tend to be the worst group empirically. We use the official train-val-split of the dataset.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-178bdcf7-1ec1-4d42-853d-9387e1de4cc4",
        "deepnote_cell_type": "markdown",
        "id": "NYIFrXjXwnFv"
      },
      "source": [
        "## Generate synthetic data \n",
        "\n",
        "Take the `mnist` dataset and consider the task of classifying digits 1 vs. 2. Alter the data such that the digit 1 is spuriously correlated with grey dots on the upper lefthand corner of the image. \n",
        "\n",
        "Concretely, inputs are `mnist` digits, labels are digit number Y = {1, 2}, and suprious attributes are grey boxes, A = {present, absent}, with added grey boxes associated with digit 2. Of the four groups, digit 1 with grey boxes are the smallest group. To mimic the `celebA` dataset in the original paper, we will alter the images such that approximately 8.5% of the training examples are digit 1 with grey boxes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-67240f2a-4e48-4cb7-bb3e-37a75dc9065d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4ec12639",
        "execution_start": 1638636999369,
        "execution_millis": 9531,
        "deepnote_cell_type": "code",
        "id": "g9akJQHRwnFv"
      },
      "source": [
        "# pip install modules\n",
        "!pip install tensorflow -q\n",
        "!pip install autograd==1.3 -q\n",
        "!pip install --upgrade tensorflow -q\n",
        "!pip install --upgrade tensorflow-gpu -q"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00004-da943132-edc1-4132-9e45-c6ad7e8c4552",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9f32aa00",
        "execution_start": 1638637017144,
        "execution_millis": 3,
        "deepnote_cell_type": "code",
        "id": "myBo2lkPwnFw"
      },
      "source": [
        "from autograd import grad\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-b14df233-3ca6-4b01-ba97-a19dccfb91e2",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d1eebf40",
        "execution_start": 1638637021366,
        "execution_millis": 31191,
        "deepnote_cell_type": "code",
        "id": "VxN_CpcywnFw"
      },
      "source": [
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version = 1, return_X_y = True, as_frame = False)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-d328ae3c-29ec-4624-959e-add463d8be93",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e7eea077",
        "execution_start": 1638637052574,
        "execution_millis": 82,
        "deepnote_output_heights": [
          611
        ],
        "deepnote_cell_type": "code",
        "id": "mGZJSpgIwnFx"
      },
      "source": [
        "# Pick out two classes of digits: 1, 2 and take a subset of samples \n",
        "X_subset = X[((y == '1') | (y == '2'))]\n",
        "y_subset = y[((y == '1') | (y == '2'))]\n",
        "\n",
        "# Encode the label '1' with y = 1, and the label '6' with y = 0\n",
        "y_subset[y_subset == '1'] = 1\n",
        "y_subset[y_subset == '2'] = 0\n",
        "y_subset = y_subset.astype(int)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-8cfcbef5-61a6-492f-b155-f6264540bd2b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8dc1b3e6",
        "execution_start": 1638637104333,
        "execution_millis": 33,
        "deepnote_output_heights": [
          155.5
        ],
        "deepnote_cell_type": "code",
        "id": "6B2uBJlZwnFx"
      },
      "source": [
        "# Randomize our data \n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X_subset.shape[0])\n",
        "X_subset = X_subset[permutation]\n",
        "y_subset = y_subset[permutation]\n",
        "X_subset = X_subset.reshape((X_subset.shape[0], -1))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00008-f6fbb3ff-d189-4699-9f30-acabbf3eedb7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4ec27f2c",
        "execution_start": 1638637108703,
        "execution_millis": 59,
        "deepnote_cell_type": "code",
        "id": "iyZtagrbwnFx"
      },
      "source": [
        "# Split into training and testing sets\n",
        "# Following celebA: 80% training, 10% val, 10% test \n",
        "\n",
        "X_train, X_test_tot, y_train, y_test_tot = train_test_split(X_subset, y_subset, test_size = 0.2) # split out train\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test_tot, y_test_tot, test_size = 0.5) # split out validation set "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00009-9b2d7995-74c1-41af-b8b3-ab5e3007eab6",
        "deepnote_output_heights": [
          218.96875
        ],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a4cdb45d",
        "execution_start": 1638637111824,
        "execution_millis": 2786,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "G9xND3IJwnFx",
        "outputId": "3dc86bd7-a8fc-4116-c90d-24e519bf189e"
      },
      "source": [
        "# Visualize some samples from the training dataset \n",
        "fig, ax = plt.subplots(2, 10, figsize = (25, 4))\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(X_train[y_train == 0][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
        "    ax[1, i].imshow(X_train[y_train == 1][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00010-c1ae040b-0b72-4ef9-b4ff-408d7867e55e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ac65e9fe",
        "execution_start": 1638637137114,
        "execution_millis": 2,
        "deepnote_cell_type": "code",
        "id": "SvccKt5IwnFy"
      },
      "source": [
        "# function to add a grey rectangular box to top left-hand corner of digit\n",
        "\n",
        "def add_spurious_ft(x_array, y_array, perc_spurious_1, perc_spurious_2):\n",
        "    '''\n",
        "    x_array: array of shape (n_images, 784)\n",
        "    y_array: array of shape (n_images, )\n",
        "    perc_spurious_1: percent of digit 1 images to add spurious features to\n",
        "    perc_spurious_2: percent of digit 1 images to add spurious features to\n",
        "\n",
        "    returns: altered image array\n",
        "    '''\n",
        "    \n",
        "    indices_1 = np.asarray(np.where(y_train == 1)).reshape(-1) # indices where digit = 1 \n",
        "    indices_2 = np.asarray(np.where(y_train == 0)).reshape(-1) # indices where digit = 2 \n",
        "    \n",
        "    num_digit_1 = len(indices_1) # number of digit 1 images \n",
        "    num_digit_2 = len(indices_2) # number of digit 2 images \n",
        "\n",
        "    num_alter_1 = int(perc_spurious_1 * num_digit_1) # number of digit 1 images to alter\n",
        "    num_alter_2 = int(perc_spurious_2 * num_digit_2) # number of digit 2 images to alter\n",
        "    \n",
        "    rand_indices_1 = np.random.choice(len(indices_1), size = num_alter_1, replace = False) \n",
        "    rand_indices_2 = np.random.choice(len(indices_2), size = num_alter_2, replace = False) \n",
        "    \n",
        "    grey_boxes = np.r_[0:15, 28:43, 56:71]\n",
        "    altered_imgs = x_array\n",
        "    \n",
        "    for i in range(len(altered_imgs)):\n",
        "        \n",
        "        # alter digit 2 images \n",
        "        if y_train[i] == int(0):\n",
        "            if i in rand_indices_2:\n",
        "                altered_imgs[i][grey_boxes] = np.array([104] * len(grey_boxes))\n",
        "        \n",
        "        # alter digit 1 images \n",
        "        elif y_train[i] == int(1):\n",
        "            if i in rand_indices_1:\n",
        "                altered_imgs[i][grey_boxes] = np.array([104] * len(grey_boxes))\n",
        "    \n",
        "    return altered_imgs"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-364bb554-b61a-4638-94e4-db0d52778dd9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7fef5598",
        "execution_start": 1638637201598,
        "execution_millis": 144,
        "deepnote_cell_type": "code",
        "id": "m3KiJIC3wnFy"
      },
      "source": [
        "# test to see if the function works \n",
        "test_altered_imgs = add_spurious_ft(X_train, y_train, 0.085, 0.7)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00012-d1888289-abc3-411a-8bc2-0f0536b4de4e",
        "deepnote_output_heights": [
          218.96875,
          224.359375
        ],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5bdeb23",
        "execution_start": 1638637205263,
        "execution_millis": 3104,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "f2Wnxij8wnFy",
        "outputId": "bf822c90-f014-4120-dff1-044dd2f3b7ca"
      },
      "source": [
        "# Visualize some samples from the altered training dataset. more 2's with spurious features than 1's. seems to work! \n",
        "fig, ax = plt.subplots(2, 10, figsize = (25, 4))\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(test_altered_imgs[y_train == 0][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
        "    ax[1, i].imshow(test_altered_imgs[y_train == 1][i].reshape((28, 28)), cmap = plt.cm.gray_r, interpolation = 'nearest')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABYEAAAD7CAYAAAA8Tlu1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7zVU/7H8c9CpTp0n266KCkVJsplSGVccokRUmFIIXcxRpEuyIipGQw1udWUcQ0dBqFfSZlJhzEupYsuRJcT0kUivr8/OrNmreXsfb57nX357u9+PR+PHn1Wa5/9/cw5b/vs/Z39/WwVBIEAAAAAAAAAAOJpt1w3AAAAAAAAAADIHE4CAwAAAAAAAECMcRIYAAAAAAAAAGKMk8AAAAAAAAAAEGOcBAYAAAAAAACAGOMkMAAAAAAAAADEWKVOAiuleiqlliilliulhqarKcQbuYEPcgMf5AY+yA18kBukiszAB7mBD3IDH+QmflQQBH5fqNTuIrJURI4XkTUislBE+gVBsCh97SFuyA18kBv4IDfwQW7gg9wgVWQGPsgNfJAb+CA38bRHJb72MBFZHgTBChERpdQTInK6iCQMRP369YOWLVtW4pBIl3feeWdjEAQNcnDolHNTVFQU1KtXL0vtZUeDBrn41lceucmtfMzNqlWrZOPGjSpHh08pN2QmOnL4WCNCbsiNn5Ryw3Pi6OC5TW7xeJMyciPkxgPnbvJYPuWGx5toSPYavDIngZuKyGfGeo2IHJ7sC1q2bCklJSWVOCTSRSm1OkeHTjk39erVk2HDhmW0qWwbPHhwrlvwQm5yKx9z07lz51wePqXckJnoyOFjjQi5ITd+UsoNz4mjg+c2ucXjTcrIjZAbD5y7yWP5lBseb6Ih2WvwjH8wnFLqEqVUiVKqpLS0NNOHQ0yYudm6dWuu20GeIDdIFZmBD3IDHzwnhg8eb+CD3MAHv6fgg8eb/FKZk8Cfi0gzY71P2b9ZgiCYFARB5yAIOufj26iRdinnpqioKGvNIbLIDXxUmBsyg3KQG/hIKTc8J4bw3AZ+yA18cO4GPni8iaHKnAReKCJtlFL7KqWqikhfESlOT1uIMXIDH+QGPsgNfJAb+CA3SBWZgQ9yAx/kBj7ITQx5zwQOgmCnUupKEZkpIruLyCNBEHyUts4QS+QGPsgNfJAb+CA38EFukCoyAx/kBj7IDXyQm3iqzAfDSRAEL4nIS2nqBQWC3MAHuYEPcgMf5AY+yA1SRWbgg9zAB7mBD3ITPxn/YDgAAAAAAAAAQO5wEhgAAAAAAAAAYoyTwAAAAAAAAAAQY5wEBgAAAAAAAIAYq9QHwwHZ0KBBAxk8eHCu20CeiXJufvjhB11/99131t59992n69dff93au+yyy6y1UkrXderUsfZGjx6t6yuvvNK/2QIS5cwgusgNgGwptMeb0tJSa71x48aEtz3ggAMy3U7eytfcfP7557qeOHGitXfyySdb65dfflnXe+65p7X3l7/8RdcDBw5MZ4vIc5999pmu99jDPjXWuHHjbLcTC/n6eJOK888/X9fTpk2z9szfU/Xq1ctaT6ngncAAAAAAAAAAEGOcBAYAAAAAAACAGGMcRBZt2bJF19u2bUt4O/dt41WqVMlYTwCyzxzP8NBDD4X+OnP8g7vetGmTtTdmzBhdH3744dZely5dQh8TQP4wn2eIiAwfPlzXTz/9tLW3du3ahPfTp0+fhPdz4IEHVqZFRMTWrVt1/fzzz1t7q1at0vWiRYusPfMS7COPPNLaa926dRo7RKG4/fbbrfWMGTN07Y5/SDYO4owzztD1sGHDrD1GRUSXORbtqaeesvbuuOMOXS9ZssTaS/aceMeOHdbexRdfrOumTZtaez179kyxY+SzRx991FoPHTpU1+5zH3NEHwrbe++9Z63N59R77bWXtbf77rtnpafK4J3AAAAAAAAAABBjnAQGAAAAAAAAgBjjJDAAAAAAAAAAxBgzgSvJnb+3YsUKXU+cONHae/PNN3W9ePHihPd56qmnWut+/frpum/fvl59Asgddwb4/Pnzve7HnX/Wpk0bXa9fv97a27Bhg65vuOEGa2/OnDlex0c0ffnll7p25+kFQWCtzQxNnjzZ2lu4cKHX8Q877DBdX3fdddaeO18N6Wc+Dzn00EOtveXLl+vandf6y1/+MuF9/uMf/7DWL7zwgq6Li4utveOOOy58s8iqn376SdfuTGjzOWoqvxMef/xxXbvzoXv37m2tr7nmGl3XqVMn9DGQ/9zXOTfffLOu3RnUyX5PpbI3bdo0XU+dOtXaM9fnnXde0t6RXXfeeaeub7311tBft+eee1rrU045Rdf77beftTd27Fhdn3322dbemjVrdF2rVq3Qx0d+KCkpsdbuY0NpaamuH374YWvvySef1LX7Guyiiy7SdatWraw983yNOy8W+eHbb7+11gMGDLDW5tzxtm3bWnv58HlevBMYAAAAAAAAAGKMk8AAAAAAAAAAEGOMg0jRBx98YK1HjRplrd1LnEz777+/rv/0pz9ZewcccICuL7/8cmvv3//+t64ZBwHkn7lz51rrZONgTO6lteeee661/s1vfqNr87IkEfvSX3NsBPLDhx9+aK03bdqk69/97ncJ95YtW2btJbuU1pVsLxlzjISbQxOjITLj4IMP1vUXX3xh7Y0bN07XgwYNsvaSXaJ4++23W+sRI0boesyYMdZejx49dL377ruH6BjZ8s033+g6E88f3efE7vqOO+7Q9SuvvGLtHXvssWnvB7llPrcxxwSJ2GOxKvpdY+7/7W9/s/bat2+va/e5lTlWwByJJSKyZMmSpMdE7pgjZpJp2bKltX722WetdadOnXT99ddfW3vm6+6tW7dae9u3b9c14yDy0+eff26thwwZous33njD2jPHP4iI1K5dW9fJxmS55s2bp2tz3IiIyN13363rpUuXhr5P5JY5Gst8bluR999/31pPmjRJ12YWo4R3AgMAAAAAAABAjHESGAAAAAAAAABijJPAAAAAAAAAABBjzAQuhzuL8Z577tG1O3/InMXomjVrlrXu0KGDrhs0aJDw64qKiqz1okWLEjeLWDDnPq9bt87aO+mkk7LdDtKsc+fO1rp169a6/uSTTxJ+Xbt27ay1OQO4IuYM8gcffDD01yGzzBm9mzdvtvaGDx+u6xkzZlh77rwzX1WqVNF1jRo1Qn9dsr5N5mw9kZ/P5UPlrVixwlqvXLlS1/fff7+1537GQFhXXXWVtZ44caKuzZlpIiIbN27UdcOGDb2Oh+xr3LixrqdPn27t3XDDDbp250yb/427z1dcP/zwg6779etn7U2ZMkXXPXv2DNExoq5+/fq6Pvroo609cya0+9zG/fnfdNNN5d6n65BDDrHWn376qa7dz14xZzTedtttCe8T2WfO+nU/12DffffV9ZNPPmntmTOAXXXq1LHW5gzyl19+2doz57eac/QRLW+//ba1Li4u1vUjjzxi7a1du1bX7gzyCRMmWOsTTzxR1+7c6WR27typ6169ell7M2fODH0/yB33tdVpp52Wlvs1PyslqngnMAAAAAAAAADEGCeBAQAAAAAAACDGCnYcxKpVq6z1rbfeqmt35IN56at7SYFr7Nixuj7mmGOsvd12S3zO3eyntLTU2nMvqUJ0mJebbNiwwdp7/PHHE37d0qVLrbV5aZJ5eYmIyIgRI3R9yy23ePWJ3HLHv5iXPrqXdpujIwYMGJDZxpB1Tz/9tK7dy6MzYejQodbavNTtkksuCX0/5mW25uWZyL758+db6xNOOEHXvuMfXLVq1bLW5jgr9/K5adOm6fr6669Py/GRHnvuuaeu3ecPtWvX1vWRRx5p7c2bNy/hfZq/syZPnmztLV682Fo/88wzunafI5m/38znUshf5nMd95L7d999V9fuOIhURhMlY+bPfb12wAEHpOUYSD9zHMP7779v7ZljHJo0aZKR47/11lsZuV9U3uuvv65rd+TCd999l/DrGjVqpOs77rjD2svEaytzZFp5a0STO3pvy5YtCW+7xx72aVNzhJ97zuf555/XtXvOxxzXmEu8ExgAAAAAAAAAYoyTwAAAAAAAAAAQY5wEBgAAAAAAAIAYi/VM4B9//NFa/+1vf9O1OQNYRGT16tUJ78ec67LffvtZe+6cmbPOOitUbz/88IO1fuCBB3T9xRdfWHvpmpWFcNwsmHOeP/jgA2tv+fLlul6/fr33Mc2MuXPMtm7d6n2/iKZ77703Lfezfft2XbtzzhFNxcXFXl9nzjN87rnnrD1z7qerWbNmXsdzH88uvfTSUF+39957W+vmzZt7HR/hmfPG3Hlme+21V1qOceihh+r61VdfTct9IvOqV6+ua/d5r69WrVolvM/p06dba3MmsMv8/IM1a9ZYe/vss09lWkQEHXLIIWm/z23btllr8/k7Mznzx0EHHVRuXRluNtx55aYuXbqk5ZhIvzFjxuh6x44d1l7Dhg113b17d2tv3Lhxum7atGlGenvkkUd07T4vOuOMMzJyTKRXsucoLjOLIiK///3vdb1o0SJrb8mSJbo2X6tHSYXvBFZKPaKU2qCU+tD4t7pKqdeUUsvK/q6T2TaRb8gNfJAb+CA38EFu4IPcwAe5QarIDHyQG/ggN4UlzDiIySLS0/m3oSIyKwiCNiIyq2wNmCYLuUHqJgu5QeomC7lB6iYLuUHqJgu5QeomC7lBaiYLmUHqJgu5QeomC7kpGBWOgwiCYK5SqqXzz6eLSPeyeoqIzBGRG9PYV1q4b80eNGhQwtvWq1dP1+5laN26ddP1NddcY+3tu+++Xr298cYb1tq8bMHsRUSkV69eXsfIpajnZt68edb6+uuv1/Xbb7/tdZ/uZdctWrTQdf/+/a29hQsXWutHH31U161bt7b2rrjiCq9+8lHUcxM1mzZt0rX7mPLLX/4y2+3kTD7lxvzvef78+dbegQceqOvBgwdbe23bttW1+xiRLt98842uL7zwQmsv2QiAoqIiXf/5z3+29k466aT0NJcB+ZQb06mnnmqtzZEP5uX/6bRixYqEe+3bt8/IMaMqX3OTDeZjmLt2x2lt3LhR1/fcc4+1d/fdd2egu9wiN+lhXtZ/5plnWnvmJbjuaLWbbrops41lAJnx991331nrZGMfzXFHcRCn3FxwwQW6dp9PXnLJJbquXbt2xnspKSmx1k888YSuq1atau317t074/2kW5xyE5Y7RsQ8d2iOexARGTJkSDZayhrfD4ZrGATB2rJ6nYg0THZjoAy5gQ9yAx/kBj7IDXyQG/ggN0gVmYEPcgMf5CamfE8Ca8GuyfsJp+8rpS5RSpUopUpKS0srezjEBLmBD3IDH8lyQ2aQCLmBD3IDH+QGqeI5MXyQG/ggN/HiexJ4vVKqsYhI2d8bEt0wCIJJQRB0DoKgc4MGDTwPh5ggN/BBbuAjVG7IDBzkBj7IDXyQG6SK58TwQW7gg9zEVIUzgRMoFpELROTOsr9npK2jNKpWrZq1njp1qq7ffPNNa8+c09ixY8eM9LNjxw5d33XXXQlv586APfroozPSTw5EJjfunGVzFqY7V2jAgAG67tChg7Vn/mzq1q1r7SV7AFy2bFnCvbPOOstaN2/ePOFtC0RkchM17nwqWCKZmyOPPFLXK1euzGEnP5+Zd8YZZ+janTGdzMMPP6xr9/ErD0UyN6Y6dewPZ/7Nb36T9mNs377dWptzODt37mzt/frXv0778fNQ5HOTDUuXLrXWyZ7rQETITcrq16+va/NxScSeA1yjRg1rL0bPpclMCMXFxaFvm4+fveMhL3Pjfj5Ftv3rX//S9emnn27tme92Pffcc6298847L7ONZU9e5iasK6+80lqbc6YbNWqU9Gt/+uknXX/77bfpbSwLKnwnsFLqcRH5p4i0VUqtUUoNlF1BOF4ptUxEjitbAxq5gQ9yAx/kBj7IDXyQG/ggN0gVmYEPcgMf5KawVPhO4CAI+iXY4q0fSIjcwAe5gQ9yAx/kBj7IDXyQG6SKzMAHuYEPclNYfMdB5IX9998/4bp///5e9/nhhx9a61RGR9xzzz26njVrlrXXpEkTXQ8cONCrN4TXpUsXa/3666/resqUKdbeaaedVunjbdmyxVq/8sorCW/brFmzSh8P8fT9999b67Fjxya87e9+97tMt4M84F6e/eSTT+p6/Pjx1t7mzZsT3k9RUZGuzfEPIiJnnnlmZVpEBH388cfW+v3339f1CSecYO1VrVo1Kz0he4YPH67rL774wtr705/+pOtatWpZe+bYM5Gfj5wBUnX77bdba/P3jzn+wV2747LatWuXge4QJeZzmD/84Q857AT56oMPPrDWJ598sq43bdpk7XXr1k3X999/f2YbQ0a4ozxT8dFHH+n6xRdftPbatGmj69atW3sfI5N8PxgOAAAAAAAAAJAHOAkMAAAAAAAAADHGSWAAAAAAAAAAiLFYzwRO5umnn7bWL7/8crm1a/v27da6evXqoY+5detWXXfo0MHamzt3rq5r164d+j7hZ8KECdb6pZde0nXPnj3TfrxbbrnFWrvzFvv06aPrvn37pv34iAfzMURE5K233tK1OX9IRKRfv0Tz/VFITjnlFGu9YsWKUF93wAEHWOsbbrhB12eddVblG0OkuZ9bYM7aPPTQQ7PdDjJg5cqVunb/ezfnzwdBYO299957ur755putvZ9++in08Rs1aqRr5nfCVFpaquvHHnvM2lu9erWu3WyeccYZumYGcOH597//retly5YlvN3BBx9srWvWrJmxnhB9//rXv3RtzgAWEfn22291fdNNN1l75u+/VM4HITrczy3429/+puv58+dbe+4MevPckevEE0/UtfmZKlHCO4EBAAAAAAAAIMY4CQwAAAAAAAAAMRa7cRDbtm3T9Ysvvmjt3XbbbbpetGhRwvv4xS9+Ya3Nt3+7b/ffsWOHtd60aVPC+zUvW9qwYYO1N23aNF1feuml1l6VKlUS3if8tG7d2lpfddVVaT+Gecnao48+mvS25mUDdevWTXsvyE/u+Ad3VIn52BTVy02QXY8//ri1/uSTT6y1ezmTyXwceuqpp6w98hVvP/zwg7X+4IMPrHWNGjV0bY4GQf4yn5O6z2WTMS+5dkfDdOnSxauXPfaI3cuRgvDss89a6969e3vdz6RJk6z1gw8+qOslS5ZYe+bvMPf10rBhw7yOj8xbvny5rl955RVrb926dbo+8MADrT3zeYk7LnHVqlXW2hynl+y5zkUXXWStq1WrlvC2iI4ZM2ZY6+LiYl27o87M32/JsiBinxNyz+M8/PDDuh4wYED4ZhEZn3/+ubUeM2aMrp9//nlrb+3atWk5ZqtWrdJyP5nEO4EBAAAAAAAAIMY4CQwAAAAAAAAAMcZJYAAAAAAAAACIsbwfwvX1119b69/85je6fvPNN609c6bdrbfeau21bNlS1+6Msz333FPX27dvt/auvvpqa/3II48k7PXggw/W9aeffmrtXXPNNbp2Z5eYvTIfOLreeustaz1q1Chdb968OenXmvOK3PnBbdq0CXX8efPmWetly5aF+jqXOf8IueU+TpWUlFhrMzeZmGuNaHL/G73zzjt17c4ANueiiYg0adJE188995y15zvPE/nvH//4h7WeOnWqtf7tb3+r6zp16mSlJ+SfhQsXhr5tjx49MtgJUlFaWmqtb7nlFl0vXrzY2ps7d66u3VmbyeZwtm3bVtft27e39tzPaTHnALu/w8444wxdT5gwQRAdP/74o67d5ynmLPktW7aEvs+mTZvq2syliD0TVkRk/fr1Ce/HfA3uvnZHdn3//ffW2nz96s79feKJJ3S9ceNGa899bEi0V9FM4GTM1+9u31WrVvW+X6TXTz/9ZK3N18/33nuvtWeeO3R/hjVr1tS1+TljqTKP6c4HPv30073vN514JzAAAAAAAAAAxBgngQEAAAAAAAAgxjgJDAAAAAAAAAAxlpczgd955x1d//73v7f2zLky5nxg97ZHHHFE6OOZcxPvvvtua2/BggUJv86cYyMicvbZZ+vanWNkzri66667Eu4ddthhITpGLgwcONBamzPNKppHdNFFF4W+bSLubCTzfqpVq2bt7b333rq+9tprvY6H8EaOHGmtzZ+NOxvovvvu0/Xf//73pPdbu3ZtXXfv3r0SHSJq3Jn25sxnd0bjzp07de0+ftx8883WesCAAbp251ShcD3zzDPW2v19cuSRR2azHeTY/vvvr+uJEydae+ZznZUrV4a+z1NOOcVa33///Z7dIR3M2b7XX3+9tWe+znJ/p5jrZM9X3T3zOfHSpUutvWTPX83XQCI/n1eO6DBfEw8ePNjaSzaj1XyNsmPHDmvP/JycSy+91NpLlj/z83xEfj4XFNllPm+9/PLLrb033nhD1+bnN4nY5z3c3zerV6/26sV93duiRQtdu3PGzdt++OGH1p75O4z5wLk1dOhQa+2erzMddNBBunZ/3r/61a90XZlZ0qtWrdJ13759rb0xY8bo+rrrrvM+RmXxTmAAAAAAAAAAiDFOAgMAAAAAAABAjOXFOAhzHIOIfTnrli1brL2xY8fq+ne/+13oY6xYsULX7iUj5tp9a3iTJk2s9ZQpU3R9zDHHJDze8ccfb63NSy3/+c9/WnvTp0/XNeMgouXLL7/U9bZt27zvp2XLlrpu06aNtVdUVBTqPi688MKEe40aNbLW5Cg9zMvULr744oS3W7hwobX+6quvdH3bbbd5H3/79u26njZtmrXXunVrXffr18/7GMic7777zlqXlJTo+txzz7X2zKy5mjdvrus//OEP1p57GRIKyw8//KDrRYsWWXvmc4snn3zS2jvppJOstfm8C/FXs2ZNXffo0cPae/DBB3V93HHHhb7Pxo0bW+s6dep4dgcf559/vrU2nzO4r23MS/fbtWtn7Znr+fPnW3sbNmxIeHx35EM69pBbmzdvttbDhg0L9XXma3URkRNPPFHXr7zyirXnXuYdlvkYJiKyzz776NocHSki0qVLF1274/Pg549//KO1Ni/Pd0cnXHHFFbp2RzW8//77uh4yZEjo43fr1k3Xw4cPt/bc8Xl77PG/02Hu46R524cfftjaO/DAA3V99dVXh+4N6WGOXR03bpy117BhQ127Y/HM1+vu6zB3lKzJHTHz2muv6frdd9+19syRD+7vxUmTJunafa1n9p1pvBMYAAAAAAAAAGKMk8AAAAAAAAAAEGOcBAYAAAAAAACAGIvsTOBevXrpevbs2daeOQdz1qxZ1l6yObzmTI7TTz/d2vv444917c44Mt13333Wun///ta6du3aCb/W5M4yNmfLujp37hzqPpF9CxYs0PWaNWusvb333lvX//73vxPuidgzqKpXr27t7b777pXuE5lhzkacOXNm1o+/Y8cOXY8cOdLaM2dJT5w40doz5wVfddVV1l6nTp3S2SIc5vyp3r17W3thM9SxY0drPXDgQF0zA7iwufMUzc9GcGcCm7M23Zmgffr0sdbuDD/Em/mc2J3t+Pjjj2e7HXh69tlndf38889be+Z/8+5//+bc3912s98vZM4B3rhxY+j7bNCgQcI9d2aiue/2bc69r8xnKqDy1q9fb63nzp2b8LZTp07V9XnnnWftmfM0n3nmGWvPd160m81WrVrp2syiiMgnn3yia2YC+7v99tt1fcsttyS83fjx4621+Tkp7kzWDz/8UNfm7F4Rkfbt2+vafe7rzgEOq27dutbafIxxe7vxxht1ffTRR1t7hxxyiNfxkZg7v/eyyy7TtXuu5Omnn9Z1165drb2VK1fq2s3pjBkzdF2lShVrz5wBLGL/zN2ff4sWLXT917/+1dr7v//7v3L/N4jYv7MzjXcCAwAAAAAAAECMcRIYAAAAAAAAAGIssuMgXnzxRV27lyGalyl2797d2tu2bZuub7jhBmvPvSTatOeee+r6lFNOsfbMt4p36dIlSdfh7dy501qbb3F3L28pKSnR9dlnn52W48PP0qVLrbV5Gbbr0ksv1fW+++6bsZ6QO59++mmuW0ho69atup43b561Z67dTJuXk5sjJeDHvMxNRGTQoEG6TmWEyAEHHKDrl156ydpr2rSpVz/mJUkiP/+daWrcuLGu33rrrdDHQ+aZl9kefvjh1p55iZo7ash8buH6+uuv09Qd8pE5di3Z4wKi7dVXX9W1+fpIJPml9IsXL9a1O7oh2RgZc8+95H7ChAm6dkchucxLYqdNm2btjRkzptzjiYjcdNNNuq5Ro0bSYyD93DyYfvrpJ12PHj3a2rv77rt1/e233ya8T/fn7Y7Wq1Wrlq7dURVnnnmmrt3Ls/faa6+EfSM883xJsixcf/31Cffcn3GPHj107Y54OPbYY1NtMWXmCAj3f5M5km/FihXWHuMg0s/9Hm/atEnX1113nbXnjoAw9ezZU9fua2DTI488Yq3dkQ/JmGNn3RG0Cxcu1PUvfvGL0PeZbrwTGAAAAAAAAABirMKTwEqpZkqp2UqpRUqpj5RS15T9e12l1GtKqWVlf9fJfLvIF+QGPsgNfJAbpIrMwAe5gQ9yAx/kBj7IDXyQm8IS5p3AO0Xk+iAI2ovIESJyhVKqvYgMFZFZQRC0EZFZZWvgv8gNfJAb+CA3SBWZgQ9yAx/kBj7IDXyQG/ggNwWkwpnAQRCsFZG1ZfUWpdRiEWkqIqeLSPeym00RkTkicmO6GjPnrrRq1craGzr0f9lz58rMnTtX1++8807C+2zdurW1Z84jcmd3ZEJpaam1NmeLujNnTjrppIz3k265yk2mTZ8+3VqvW7dO1x07drT2xo4dm5We4iSuufF1/vnnW2t3VpY7K8/HP//5T2ttZnq//far9P1nQ9Rys2XLFl0PHjzY2psxY0ao+xg1apS1Pvnkk3X9448/Wnvmz/APf/iDtedmxpz16v7sTe48xWQz3PJR1DJTEXP2nPu48PTTT+va/O9XxP695M5BrFu3rq7deaHufLUlS5bo+uKLL7b29t9/f10ny5SrW7duuq5WrVror8ulfMtNMs2aNdP1+PHjrb2bb75Z1+Z84Mp48sknrbU5k65G02oAACAASURBVPq5556z9lq2bJmWY0ZFFHKTbEan71779u2t9RlnnKFr93GiefPmFbWomTODzfmNIva84N/+9rfWXrt27XR93nnnhT5eVEUhN8lUqVLFWpufr2N+1o2IyAUXXKDrZJlKxp2zav7uExGpXbu2rr/55htrr5A+myXquTFnN4vYP1d37q85h9XNW67Vr19f1+5jYT6Kem7Mz1pyLViwwFqbzy/NGbwi9mNT27Ztrb2//OUvuj7uuOO8+qxIuj5frLJSmgmslGopIp1EZIGINCwLi4jIOhFpmNbOEBvkBj7IDXyQG6SKzMAHuYEPcgMf5AY+yA18kJv4C30SWClVJCLTReTaIAg2m3vBrrcalfsxs0qpS5RSJUqpEvfdr4g/cgMf5AY+fHJDZgobjzXwQW7gg9zAB7mBD3IDH+SmMFQ4DkJERClVRXaF4bEgCP57Hc56pVTjIAjWKqUai8iG8r42CIJJIjJJRKRz587lhqYiS5cutdbu5SCJuJcXmpfnn3vuudZevXr1fFrzttdee1nrE044QdcHHHCAtferX/0qKz2lW65zky6rV6/WtXs5bYMGDXRtXj4Jf3HJTVgdOnSw1r///e917T5OuZf233XXXaGO4V6Gu3LlSl1v3brV2svXy3B9c5OJzPTp00fXr776qtd9fPTRR9Z65syZuk7lkns3M+ZlmNWrV7f2RowYoWt3ZNKZZ54Z+pj5Ip8ea8wxHs8884y117lzZ11PnTrV2jN/L7mX4Jq/z3bu3GntjRkzxlpPmjRJ15MnT7b2zOczX375Zbn9i/z8su6jjjpK1/kyDkIkv3KTjHlp7ZAhQ6y9hg3/90afTZs2hb7P9957z1o/+OCDujbH5Li3NccIiNjjAG655ZbQx4+yXOTGfL1kvs4QEdm4cWOo+3BfkwwbNizhXia4o4kOPfRQXU+cONHae/fdd3Udh3EQItF+vHGfL5rf84ceesjrPs2REiIiV111la5vuOEGa8+8HN9ljjsqRLnIzVNPPWUeP+HtjjjiCGu9zz77hD1E1pn5c0ecXHvttbqOwzgIkWg/3rhjFObNm6fr+fPnh76fTp066dod81lIY2MqfCew2vVf8cMisjgIAnNoWLGI/HfAzwUiEm7QIQoCuYEPcgMf5AapIjPwQW7gg9zAB7mBD3IDH+SmsIR5J/BRInK+iHyglPrv/21/k4jcKSJPKaUGishqEemT4OtRmMgNfJAb+CA3SBWZgQ9yAx/kBj7IDXyQG/ggNwWkwpPAQRDME5FE7+n/dXrbQVyQG/ggN/BBbpAqMgMf5AY+yA18kBv4IDfwQW4KS6iZwFHWq1cva33cccfp+thjj7X2ojSvpVWrVtb65ZdfzlEnqEiLFi10vWrVqtw1gsgx53VPmTLF2jvnnHN0bT4uuXt77GE/DCebjenO2DLnNiZz9dVXJ9z7/vvvrbXbD1Jnzu9NNhctmaeffjpd7VhOPfVUXZvzzEREevTokZFjIrNKSkp0/c4771h7RUVFujZneYqInHXWWQnv050B/dJLL+nanb1Wp04dXZ9yyikJ7/Oggw5KuIdo6d+/v9fXmTP6RES++OILXX/22WfW3vvvv69rd5awuTdu3LiEx/vjH/9orQcNGhS+2QJwySWXlFvnM3MOcTZmEiO88eP/dwW5+7p2zZo1Cb/u4IMP1vW9995r7XXt2jVN3SHTzj777Fy3kHZuHpE7d955p7V+7rnndO2en+nYsaOuzc/aEbGf3+y+++5p7DC/VDgTGAAAAAAAAACQvzgJDAAAAAAAAAAxFtnrfteuXavrZJfT1q1b11pzKTOAbDEvPc3Xy1CrVq2a6xaQIvMyJxGR3Xb73/+fa44aERE5+uijrfUhhxyi6xo1amSgO2RC9erVdd2kSZOEt3NHz1x++eW6Puyww0Ifr0qVKtb69NNPL7cGTO7jzYsvvqjrTz75xNpbsGCBrs1xIyIijz32mK6/+eYba2///ffX9THHHOPfLIC0MscPueNfAKAy3NerK1euzFEn8cA7gQEAAAAAAAAgxjgJDAAAAAAAAAAxxklgAAAAAAAAAIixyA7QbdiwYa5bAAAgZQ888ICu77jjDmvv888/D3UfV1xxhbXu0KGDrgcMGGDtMdc5/mrVqqXrNWvW5LATwE/r1q0Trvv372/tTZs2LSs9AQAAFBreCQwAAAAAAAAAMcZJYAAAAAAAAACIsciOgwAAIB8NHjy43BoAAAAAgFzhncAAAAAAAAAAEGOcBAYAAAAAAACAGOMkMAAAAAAAAADEGCeBAQAAAAAAACDGOAkMAAAAAAAAADHGSWAAAAAAAAAAiDFOAgMAAAAAAABAjHESGAAAAAAAAABijJPAAAAAAAAAABBjnAQGAAAAAAAAgBhTQRBk72BKlYrIahGpLyIbs3bg5Aq1lxZBEDTI0rEqpSw32yQ6PycRchN55KZC2eol3zLD76jkyI2D3IRCbhzkJhRy4yA3FeI5cTnITYXITTnITYXITTl4DV6hnD+3yepJYH1QpUqCIOic9QOXg17yQ9S+N1HqJ0q9RE3UvjdR6idKvURNlL43UepFJHr9REmUvjdR6kUkev1ESZS+N1HqRSR6/URJlL439JI/ovT9oZf8EaXvD73kh6h9b6LUTxR6YRwEAAAAAAAAAMQYJ4EBAAAAAAAAIMZydRJ4Uo6OWx56yQ9R+95EqZ8o9RI1UfveRKmfKPUSNVH63kSpF5Ho9RMlUfreRKkXkej1EyVR+t5EqReR6PUTJVH63tBL/ojS94de8keUvj/0kh+i9r2JUj857yUnM4EBAAAAAAAAANnBOAgAAAAAAAAAiLGsngRWSvVUSi1RSi1XSg3N5rHLjv+IUmqDUupD49/qKqVeU0otK/u7TpZ6aaaUmq2UWqSU+kgpdU0u+4kycqOPSWZSkMvcRCUzZcclNykgN/q45CYF5EYfl9yExHMbqxdyExK5sXohNyGRG6sXchMSubF6ITch8ZxYHzeymcnaSWCl1O4icr+InCQi7UWkn1KqfbaOX2ayiPR0/m2oiMwKgqCNiMwqW2fDThG5PgiC9iJyhIhcUfb9yFU/kURuLGQmpAjkZrJEIzMi5CY0cmMhNyGRGwu5CSECmREhN3mH3PwMuQmB3PwMuQmB3PwMuQkhArmZLGSmYkEQZOWPiBwpIjON9TARGZat4xvHbSkiHxrrJSLSuKxuLCJLst1T2bFniMjxUeknKn/IDZnJ19xEMTPkhtyQG3JDbnL/JwqZITf594fckBtyQ27ITTR+TuQmurkhMxX/yeY4iKYi8pmxXlP2b7nWMAiCtWX1OhFpmO0GlFItRaSTiCyIQj8RQ27KQWYqFMXc5PznRG4qRG7KQW4qRG7KQW6SimJmRCLwcyI3SZGbBMhNUuQmAXKTFLlJgNwkFcXc5PxnFLXM8MFwhmDX6fggm8dUShWJyHQRuTYIgs257gepy/bPiczkPx5r4IPcwAe5gQ9yAx/kBj7IDXyQG6SKzOySzZPAn4tIM2O9T9m/5dp6pVRjEZGyvzdk68BKqSqyKxCPBUHwbK77iShyYyAzoUUxNzzWRB+5MZCb0MiNgdyEEsXMiJCbqCM3DnITCrlxkJtQyI2D3IQSxdyQGUc2TwIvFJE2Sql9lVJVRaSviBRn8fiJFIvIBWX1BbJrVkfGKaWUiDwsIouDIBif634ijNyUITMpiWJueKyJPnJThtykhNyUITehRTEzIuQm6siNgdyERm4M5CY0cmMgN6FFMTdkxpXNAcQicrKILBWRT0Tk5mweu+z4j4vIWhH5QXbNJxkoIvVk16fyLROR10WkbpZ6OVp2vfX7fRF5r+zPybnqJ8p/yA2ZybfcRCUz5IbckBtyQ26i+YfnNuSG3JAbckNuovyH3JCbfMsNmQn3R5U1CAAAAAAAAACIIT4YDgAAAAAAAABijJPAAAAAAAAAABBjlToJrJTqqZRaopRarpQamq6mEG/kBj7IDXyQG/ggN/BBbpAqMgMf5AY+yA18kJv48Z4JrJTaXXYNfD5edg1dXigi/YIgWJS+9hA35AY+yA18kBv4IDfwQW6QKjIDH+QGPsgNfJCbeNqjEl97mIgsD4JghYiIUuoJETldRBIGon79+kHLli0rcUikyzvvvLMxCIIGOTg0uclj5AapWrVqlWzcuFHl6PAp5YbMREcOH2tEyE3eIjfwwXMb+Min3BQVFQX16tXLUnvZ0aBBrh7qKyefcsPjTXSQG6Qq2WvwypwEbioinxnrNSJyeLIvaNmypZSUlFTikEgXpdTqHB2a3OQxcoNUde7cOZeHTyk3ZCY6cvhYI0Ju8ha5gQ+e28BHPuWmXr16MmzYsIw2lW2DBw/OdQte8ik3PN5EB7lBqpK9Bs/4B8MppS5RSpUopUpKS0szfTjEBLmBD3KDVJEZ+CA38EFu4IPcwIeZm61bt+a6HeQJHm/gg9zkl8qcBP5cRJoZ633K/s0SBMGkIAg6B0HQOV8v20BakRv4IDfwUWFuyAzKQW7gg9wgVTy3gY+Uc1NUVJS15hBZPN7AB7mJocqcBF4oIm2UUvsqpaqKSF8RKU5PW4gxcgMf5AY+yA18kBv4IDdIFZmBD3IDH+QGPshNDHnPBA6CYKdS6koRmSkiu4vII0EQfJS2zhBL5AY+yA18kBv4IDfwQW6QKjIDH+QGPsgNfJCbeKrMB8NJEAQvichLaeoFBYLcwAe5gQ9yAx/kBj7IDVJFZuCD3MAHuYEPchM/lToJDAAAcmvnzp26vvHGG629++67z1q/++67uu7YsWNmGwMAAAAAREZlZgIDAAAAAAAAACKOk8AAAAAAAAAAEGOcBAYAAAAAAACAGGMmMAAAeeT777+31oMGDdL11KlTrT2llLXeunVr5hoDAACR06BBAxk8eHCu2wAARADvBAYAAAAAAACAGOMkMAAAAAAAAADEGOMgMmjHjh3Wul+/frp+/vnnrb0WLVroeuXKlZltDACQV8wRECNGjLD2zBEQRxxxhLX38ccfW2t3HyjPqFGjrPXo0aN1HQRBlrtBVB1zzDHWeu7cuTnqBLDNmTPHWvfo0UPXs2fPtva6d++ehY4AZMqsWbOs9XHHHafrww47zNp75plnrHWzZs0y1xgQUbwTGAAAAAAAAABijJPAAAAAAAAAABBjnAQGAAAAAAAAgBhjJnAGfffdd9Z6xowZuq5fv761N27cuKz0hHgpLS3V9aBBg6y94uLihF9322236Xr48OHpbwzeXnzxRV336tXL2hs5cqS1dud2Ir5mzpyp67Fjx1p75pzfAQMGWHs33nhjZhtDLL3xxhsJ99xZm8zTjDf3uewLL7yg65NPPjnb7QAJmY9N5gzgZLcT4TGsEKxfv17XjRo1svbatm2ra3e2bNOmTTPbGNJCKZVwvXDhQmvPfX5z3nnnZa4xFIQ777zTWt900026Hj9+vLV37bXXZqWnivBOYAAAAAAAAACIMU4CAwAAAAAAAECMMQ4izXbu3Knrv/71rwlvd+SRR1rr3r17Z6wnxJc5AsK8RFPk55fGmNatW5exnlA5Q4YMSbi3evXqLHaCKDnkkEN0feGFF1p75qVFf/7zn629Tp06ZbQvxJN7uXSyPS6ljrfNmzdb6759++q6evXq1t7QoUOz0hMg8vORWKNHj05429mzZ+uax6zCM2HCBF27r4+WLl2qa3Mkm4jIpZdemtnGkHXu+MQmTZro+thjj812O8hThx9+uK7ff/99ay/ZOZio4J3AAAAAAAAAABBjnAQGAAAAAAAAgBjjJDAAAAAAAAAAxBgzgdPs1ltv1fWYMWNy2AniYsqUKbp2M7Vs2TJdJ5s/U7t2bWt9zjnnpKk7pNtPP/2UcK9FixZZ7ARR0rRpU10/+uijCW/3r3/9y1ofccQRGesJ8eHO1gTC2L59e65bQIHp0aOHrpPNLh85cqS1Zg5wYXHnQ992220Jb9u6dWtd9+/fP2M9IRq+//57a/3EE0/ompnACMv8fCU3U7/4xS90HdVzLrwTGAAAAAAAAABijJPAAAAAAAAAABBjjINIs+Li4lC369ChQ4Y7Qb7atGmTtX7xxRd1vXz58tD3Y46AeOihh6y9rl27enYHIMrMy5OAsN54443Qt+WyavxXv379ct0CYi6VUTXmCAgepwrPBx98oOsHHnjA2guCQNdVqlSx9h577DFd77XXXhnqDkA+M8/HiIh89dVXuq5WrZq1d/XVV+u6cePGmW3ME+8EBgAAAAAAAIAY4yQwAAAAAAAAAMQYJ4EBAAAAAAAAIMaYCVxJ7nyQ1atXJ7xtnz59dH3LLbdkrCfkt8suu8xaT58+3et+Jk6cqOvevXtXqidEQ0lJSa5bQASZudi6dau117Zt22y3gzw0Z86c0Ldl1mZhM2dr/v3vf7f2pk2blu12EHOjR48OfdtU5gcjfoYPH67r0tJSa8+cAzxu3Dhr77DDDstsY8i4ww8/3FqfeeaZuvZ9HQ2Y3MeNb7/9Vte/+tWvrL1hw4ZlpafKqPCdwEqpR5RSG5RSHxr/Vlcp9ZpSalnZ33Uy2ybyDbmBD3IDH+QGPsgNfJAb+CA3SBWZgQ9yAx/kprCEGQcxWUR6Ov82VERmBUHQRkRmla0B02QhN0jdZCE3SN1kITdI3WQhN0jdZCE3SN1kITdIzWQhM0jdZCE3SN1kITcFo8JxEEEQzFVKtXT++XQR6V5WTxGROSJyYxr7yhv333+/td68ebOuq1evbu0NHjw44V7ckJvk3MuUBg0apOvi4uLQ91O/fn1dP/zww9beaaed5tld7pAbkebNm+t6xYoV1t6BBx6Y7XbyQqHnxhwBsXPnTmvv008/zXY7eaPQcwM/hZ4bpVSuW8hLhZ6bVITNmDmaJI7ITHLm62oRkTfffDPhbevWravrK6+8MmM9RUEh5qZmzZrWeu+9985RJ/mrEHOTzHXXXWet3bFp5u+prl27ZqOltPL9YLiGQRCsLavXiUjDNPWDeCM38EFu4IPcwAe5gQ9yAx/kBqkiM/BBbuCD3MSU70lgLdj1f8Mm/L9ilVKXKKVKlFIl7rsfUbjIDXyQG/hIlhsyg0TIDXyQG/ggN0gVz4nhg9zAB7mJF9+TwOuVUo1FRMr+3pDohkEQTAqCoHMQBJ0bNGjgeTjEBLmBD3IDH6FyQ2bgIDfwQW7gg9wgVTwnhg9yAx/kJqYqnAmcQLGIXCAid5b9PSNtHUWcOx9k5syZ1tqcD3L00Udbe926dctcY/mhYHPjMmcAi4i88MILuk5l3l6XLl10nY8zgEMiN2WqVauW6xbyCbkRkXbt2uW6hXxTMLkZNWpU6Nt27949Y33ERMHkBmlVkLlxZyv26NEj9NfOnj07zd3knYLMjIjI22+/ba3/+te/Wmvz9VOVKlWsvUmTJmWusfxQsLlBpRRUbr799ltdL1++3Npzz8+0aNFC1wMGDMhsYxlQ4TuBlVKPi8g/RaStUmqNUmqg7ArC8UqpZSJyXNka0MgNfJAb+CA38EFu4IPcwAe5QarIDHyQG/ggN4WlwncCB0HQL8HWr9PcC2KE3MAHuYEPcgMf5AY+yA18kBukiszAB7mBD3JTWHzHQRSUN998U9dPPfVU0tsWFRXp+pprrslYT4i+TZs26fqyyy6z9oqLi611shEQtWvX1vXEiROtva5du1amRUTQp59+musWkGcWLFiQcK958+ZZ7AT5ZPTo0aFvyzgrAOnijoMAEnnvvfd0ncrr6uuvv95a9+rVK209AYgnc0zaP/7xj6S3feyxx3Tdpk2bTLWUMb4fDAcAAAAAAAAAyAOcBAYAAAAAAACAGOMkMAAAAAAAAADEGDOBQxg+fLiu165dm/S2nTp10vVJJ52UsZ4QPeYMYBGRiy++WNfTp08PfT/mDGARkYceekjXvXv39uwO+eLUU0/V9b333mvtzZ8/P9vtIA+sWLEi4d7RRx+dxU4QZeass2x+LfJb9erVrfV+++2n6+XLl2e7HcRAKvPIgyDIYCeIOnN+dLLPPxAROeuss3R90003ZaolADGxdOlSa53ss7/23ntva92qVauM9JQtvBMYAAAAAAAAAGKMk8AAAAAAAAAAEGOMgyjHtGnTrPWbb76Z8LbuZUp/+ctfMtITou+yyy6z1qmMgLj88st1fc4551h7Xbt2rVxjyCs1a9ZMuHfUUUdlsRPki1dffVXX7jiZ+vXrZ7sdRNQbb7wR+rYjR47MYCfIZx06dND1nnvuae199dVX1rpu3bpZ6QnRp5QKfdvZs2dnsBNE2caNG631hAkTEt62Vq1a1nrEiBG6LioqSm9jAGLnyy+/tNafffZZwtsOHDjQWjdq1CgjPWUL7wQGAAAAAAAAgBjjJDAAAAAAAAAAxBgngQEAAAAAAAAgxpgJXI7ddrPPjSebY9WkSRNrXaNGjYz0hGgoLS211oMGDdJ1cXFx6Ptx53SecMIJumYGMIBkPv/8c2v99ddf6/rYY4/NdjvIE3PmzMl1C8hD27dvt9YzZsxIeNsnnnjCWpufd4DCMmrUKO+v7d69e9r6QH4ZNmyYtV66dGnC215wwQXWumPHjhnpCdH3xRdfWOtly5blqBPkM/ezvkzjxo3LYieZxzuBAQAAAAAAACDGOAkMAAAAAAAAADHGSWAAAAAAAAAAiDFmApdj8uTJoW97zjnnWOtWrVqluRtEiTkDWETkhRde0HWy2dHuXDxzBrCIyGmnnZaG7hB38+fPz3ULiAB3Nvk333yTo04Qdb5zOZnJif+qWbOmtT700EN1vWjRImtvwIABWekJ0WQ+3owePTrh7UaOHJnw61B4zPmtr732mrVnvrZq3ry5tXfPPfdktjHkjY8//thaz5s3L+Ft3dfrtWrVykhPiKbPPvtM10OGDLH2zGyMGDEiaz3lAu8EBgAAAAAAAIAY4yQwAAAAAAAAAMQY4yDKzJkzR9dvvfVW6K8bN25cBrpBtn3//fe6Xr16tbVnXg5QXFxs7SUbAVG7dm1du2NDunbt6tUnCttRRx2V6xYQAckuc9tjD36t43+SXZKdDOMg8F/uOIi77rpL15deeqm1N3fuXGt94oknZq4x5Jw7xiHs4w2PLzCZ40E+/fTThLcbPnx4NtpBzFWrVs1a33333TnqBLlw7rnn6nrhwoUJbxf3MSG8ExgAAAAAAAAAYoyTwAAAAAAAAAAQY5wEBgAAAAAAAIAYK9jhgdu3b7fWEyZMSLhnGjNmTMZ6Qu6Yc4Dbtm3rdR8XXnihtT711FN1zQxgpMOSJUty3QJyZOfOnbp+4IEHEt7u6quvzkY7iBlzJiMQ1vLly631jBkzrDUzgePH/AyVVGaOz549W9fMBC5s//nPf6z1Cy+8kPC27dq10/VZZ52VsZ4AFAb3s59M9evX1/Upp5ySjXZyhncCAwAAAAAAAECMcRIYAAAAAAAAAGKsYMdBuJdVP/PMMwlvW7VqVV3vs88+GesJuTNixAivr7v88st1fdddd1l7NWrUqFRPKEz77bdfwr133nnHWpsjAvbYo2AfzmPjgw8+0PVbb71l7X388ce6Xrx4ccL7CIIg4X2KiNSqVUvXzZs3T3g/r776qrU+/PDDy70PxMOoUaNy3QLyRJs2bXTdsGFDa88crSYics455+i6W7dumW0MWdGjR49Qt3NHzDACorBt2bJF1+4YkW3btiX8OnPEVe3atdPfGACU6du3r67N5zpxVOE7gZVSzZRSs5VSi5RSHymlrin797pKqdeUUsvK/q6T+XaRL8gNfJAb+CA3SBWZgQ9yAx/kBj7IDXyQG/ggN4UlzDiInSJyfRAE7UXkCBG5QinVXkSGisisIAjaiMissjXwX+QGPsgNfJAbpIrMwAe5gQ9yAx/kBj7IDXyQmwJS4UngIAjWBkHwblm9RUQWi0hTETldRKaU3WyKiPwmU00i/5Ab+CA38EFukCoyAx/kBj7IDXyQG/ggN/BBbgpLSkMklVItRaSTiCwQkYZBEKwt21onIg0TfFkkpTIDtl27dro+77zzMtFOrEUlN6WlpboeNGiQtVdcXBzqPoYPH26tb7vttso3hnJFJTfZdvzxx+vanX+2bNkya/3RRx/p+uCDD85sY3kin3JjzvkVEenSpYuud+zY4XWfxxxzjLVWSlnroqIiXderV8/aa9Soka7ffvtta69x48a6njNnjrWXbI51PsinzFSE2b7ZE6fcpKJp06a6dh9D1q1bZ61nzJiha2YC75JvuQk7A1jEngPMY1F65VtuXObz1eeffz7h7U4++WRr3b9//4z1VAjyPTdhuZ9VYc6rX79+vbXnfnbG8uXLdZ3vz2fTJc65MX/+5msbEZGLLroo2+3kTJhxECIiopQqEpHpInJtEASbzb1g13czSPB1lyilSpRSJeZJOBQGcgMf5AY+fHJDZgobjzXwQW7gg9zAB7mBD3IDH+SmMIQ6CayUqiK7wvBYEATPlv3zeqVU47L9xiKyobyvDYJgUhAEnYMg6NygQYN09Iw8QW7gg9zAh29uyEzh4rEGPsgNfJAb+CA38EFu4IPcFI4Kx0GoXdeSPiwii4MgGG9sFYvIBSJyZ9nfM8r58kiZOXOmrl999dWEt2vZsqW1njJlSvk3REJRzE1JSYmuX3jhBWvPvWTadPnll+ua8Q+ZFcXcZFuzZs10bV7OJCKyadOmbLeTF/I1N+aoIRH7krXnnnvO2kv2GJXMF198Ya1Xrlyp640bN1p75u/FX/7yl9Ze69atdW2OlMhX+ZqZdOrevXuuW8g75CY17uNPocq33JgjINzxP8nwmJJe+ZYbk/t8dciQIQlvW7VqVV27Y0T23nvvtPZVCPI5N74OPfRQa22OFXn00UetPXfc2l133aXrSZMmZaC7/BDX3Dz44IPW2nyttdtu9vthP/vsM13HfcximJnAR4nIW+GenAAACAdJREFU+SLygVLqvbJ/u0l2BeEppdRAEVktIn0y0yLyFLmBD3IDH+QGqSIz8EFu4IPcwAe5gQ9yAx/kpoBUeBI4CIJ5IpLoLUi/Tm87iAtyAx/kBj7IDVJFZuCD3MAHuYEPcgMf5AY+yE1hCf3BcAAAAAAAAACA/BNmHERsjB07Vtc//PBDwtsdddRR1vqggw7KWE/InosuuijhXrVq1XR91VVXWXvufCogW/bdd19rvWTJEmv9448/ZrMdZFitWrV0feGFF+auERSE2bNn57oF5LkmTZpY648++ihHnaAy3Lm/YecA7/qgeODnzDmrIiILFixIeNvzzz9f1507d85YT0B5pk6dqmsziyIiXbt2zXY7SLOtW7da6507d+ranTles2bNrPQUBbwTGAAAAAAAAABijJPAAAAAAAAAABBjBTUOYuXKlaFu16cPH3oYR+vWrdO1Uvbc8xYtWujavYQJyJXDDjvMWr/yyivWev369dlsB0DEde/e3VqPHj1a1yNHjsxyN4i7xx9/3Fr36NHDWn/yySe63r59u7VXvXr1zDWGlCQb/+A+pjBGBuk2ZsyYXLeAmLnkkkt0bY5aExH585//bK2PPPJIXe+///6ZbQyR0rFjR2vtPoeJM94JDAAAAAAAAAAxxklgAAAAAAAAAIgxTgIDAAAAAAAAQIwxExgF4+WXX064V7NmzSx2AoTToUMHa923b19r3b59+2y2AyDi3PmdQRDkphEUhLp161rr//znPznqBJUxatSopGsgVZ06dUq4N2LECGtdr169TLeDAnP44YeXW4uIjB8/PtvtIIe6detmrc0Z0b169cp2O5HBO4EBAAAAAAAAIMY4CQwAAAAAAAAAMVZQ4yBQ2Hr27JnrFoCU9OnTJ+kaAAAAiJKzzz476RoAsuGQQw6x1l999VWOOokW3gkMAAAAAAAAADHGSWAAAAAAAAAAiDFOAgMAAAAAAABAjHESGAAAAAAAAABijJPAAAAAAAAAABBjnAQGAAAAAAAAgBjjJDAAAAAAAAAAxBgngQEAAAAAAAAgxjgJDAAAAAAAAAAxxklgAAAAAAAAAIgxFQRB9g6mVKmIrBaR+iKyMWsHTq5Qe2kRBEGDLB2rUspys02i83MSITeRR24qlK1e8i0z/I5Kjtw4yE0o5MZBbkIhNw5yUyGeE5eD3FSI3JSD3FSI3JSD1+AVyvlzm6yeBNYHVaokCILOWT9wOeglP0TtexOlfqLUS9RE7XsTpX6i1EvUROl7E6VeRKLXT5RE6XsTpV5EotdPlETpexOlXkSi10+UROl7Qy/5I0rfH3rJH1H6/tBLfoja9yZK/UShF8ZBAAAAAAAAAECMcRIYAAAAAAAAAGIsVyeBJ+XouOWhl/wQte9NlPqJUi9RE7XvTZT6iVIvUROl702UehGJXj9REqXvTZR6EYleP1ESpe9NlHoRiV4/URKl7w295I8ofX/oJX9E6ftDL/khat+bKPWT815yMhMYAAAAAAAAAJAdjIMAAAAAAAAAgBjL6klgpVRPpdQSpdRypdTQbB677PiPKKU2KKU+NP6trlLqNaXUsrK/62Spl2ZKqdlKqUVKqY+UUtfksp8oIzf6mGQmBbnMTVQyU3ZccpMCcqOPS25SQG70cclNSDy3sXohNyGRG6sXchMSubF6ITchkRurF3ITEs+J9XEjm5msnQRWSu0uIveLyEki0l5E+iml2mfr+GUmi0hP59+GisisIAjaiMissnU27BSR64MgaC8iR4jIFWXfj1z1E0nkxkJmQopAbiZLNDIjQm5CIzcWchMSubGQmxAikBkRcpN3yM3PkJsQyM3PkJsQyM3PkJsQIpCbyUJmKhYEQVb+iMiRIjLTWA8TkWHZOr5x3JYi8qGxXiIijcvqxiKyJNs9lR17hogcH5V+ovKH3JCZfM1NFDNDbsgNuSE35Cb3f6KQGXKTf3/IDbkhN+SG3ETj50RuopsbMlPxn2yOg2gqIp8Z6zVl/5ZrDYMgWFtWrxORhtluQCnVUkQ6iciCKPQTMeSmHGSmQlHMTc5/TuSmQuSmHOSmQuSmHOQmqShmRiQCPydykxS5SYDcJEVuEiA3SZGbBMhNUlHMTc5/RlHLDB8MZwh2nY4PsnlMpVSRiEwXkWuDINic636Qumz/nMhM/uOxBj7IDXyQG/ggN/BBbuCD3MAHuUGqyMwu2TwJ/LmINDPW+5T9W66tV0o1FhEp+3tDtg6slKoiuwLxWBAEz+a6n4giNwYyE1oUc8NjTfSRGwO5CY3cGMhNKFHMjAi5iTpy4yA3oZAbB7kJhdw4yE0oUcwNmXFk8yTwQhFpo5TaVylVVUT6ikhxFo+fSLGIXFBWXyC7ZnVknFJKicjDIrI4CILxue4nwshNGTKTkijmhsea6CM3ZchNSshNGXITWhQzI0Juoo7cGMhNaOTGQG5CIzcGchNaFHNDZlzZHEAsIieLyFIR+UREbs7mscuO/7iIrBWRH2TXfJKBIlJPdn0q3zIReV1E6mapl6Nl11u/3xeR98r+nJyrfqL8h9yQmXzLTVQyQ27IDbkhN+Qmmn94bkNuyA25ITfkJsp/yA25ybfckJlwf1RZgwAAAAAAAACAGOKD4QAAAAAAAAAgxjgJDAAAAAAAAAAxxklgAAAAAAAAAIgxTgIDAAAAAAAAQIxxEhgAAAAAAAAAYoyTwAAAAAAAAAAQY5wEBgAAAAAAAIAY4yQwAAAAAAAAAMTY/wPjCUdKKriZMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00013-7e4d1d17-1876-43ec-947d-5010ab5e0aa9",
        "deepnote_cell_type": "markdown",
        "id": "DZea_7vWwnFy"
      },
      "source": [
        "## Generate 4 Sets of Image Datasets (Train, Val, Test): \n",
        "\n",
        "`X_train_9`, `X_val_9`, `X_test_9`: 90% spurious features in digit 2; 8.5% spurious features in digit 1.\n",
        "\n",
        "`X_train_7`, `X_val_7`, `X_test_7`: 70% spurious features in digit 2; 8.5% spurious features in digit 1.\n",
        "\n",
        "`X_train_5`, `X_val_5`, `X_test_5`: 50% spurious features in digit 2; 8.5% spurious features in digit 1.\n",
        "\n",
        "`X_train_3`, `X_val_3`, `X_test_3`: 30% spurious features in digit 2; 8.5% spurious features in digit 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-803938e5-456e-49a1-ab53-63fb32526161",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f54d4220",
        "execution_start": 1638637213521,
        "execution_millis": 830,
        "deepnote_cell_type": "code",
        "id": "ZjmdH2NmwnFz"
      },
      "source": [
        "# 90% spurious in digit 2 \n",
        "X_train_9 = add_spurious_ft(X_train, y_train, 0.085, 0.9)\n",
        "X_val_9 = add_spurious_ft(X_val, y_val, 0.085, 0.9)\n",
        "X_test_9 = add_spurious_ft(X_test, y_test, 0.085, 0.9)\n",
        "\n",
        "# 70% spurious in digit 2 \n",
        "X_train_7 = add_spurious_ft(X_train, y_train, 0.085, 0.7)\n",
        "X_val_7 = add_spurious_ft(X_val, y_val, 0.085, 0.7)\n",
        "X_test_7 = add_spurious_ft(X_test, y_test, 0.085, 0.7)\n",
        "\n",
        "# 50% spurious in digit 2 \n",
        "X_train_5 = add_spurious_ft(X_train, y_train, 0.085, 0.5)\n",
        "X_val_5 = add_spurious_ft(X_val, y_val, 0.085, 0.5)\n",
        "X_test_5 = add_spurious_ft(X_test, y_test, 0.085, 0.5)\n",
        "\n",
        "# 30% spurious in digit 2 \n",
        "X_train_3 = add_spurious_ft(X_train, y_train, 0.085, 0.3)\n",
        "X_val_3 = add_spurious_ft(X_val, y_val, 0.085, 0.3)\n",
        "X_test_3 = add_spurious_ft(X_test, y_test, 0.085, 0.3)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI9tpvHSJUKS",
        "outputId": "088a3051-7c93-466f-dccd-3f1db52b42de"
      },
      "source": [
        "# 90% spurious in digit 2 reshaped\n",
        "\n",
        "X_train_9_reshaped = X_train_9.reshape((X_train_9.shape[0], 28, 28))\n",
        "X_val_9_reshaped = X_val_9.reshape((X_val_9.shape[0], 28, 28))\n",
        "X_test_9_reshaped = X_test_9.reshape((X_test_9.shape[0], 28, 28))\n",
        "\n",
        "# 70% spurious in digit 2 reshaped\n",
        "X_train_7_reshaped = X_train_7.reshape((X_train_7.shape[0], 28, 28))\n",
        "X_val_7_reshaped = X_val_7.reshape((X_val_7.shape[0], 28, 28))\n",
        "X_test_7_reshaped = X_test_7.reshape((X_test_7.shape[0], 28, 28))\n",
        "\n",
        "\n",
        "# 50% spurious in digit 2 reshaped\n",
        "\n",
        "X_train_5_reshaped = X_train_5.reshape((X_train_5.shape[0], 28, 28))\n",
        "X_val_5_reshaped = X_val_5.reshape((X_val_5.shape[0], 28, 28))\n",
        "X_test_5_reshaped = X_test_5.reshape((X_test_5.shape[0], 28, 28))\n",
        "\n",
        "\n",
        "# 30% spurious in digit 2 reshaped\n",
        "\n",
        "\n",
        "X_train_3_reshaped = X_train_3.reshape((X_train_3.shape[0], 28, 28))\n",
        "X_val_3_reshaped = X_val_3.reshape((X_val_3.shape[0], 28, 28))\n",
        "X_test_3_reshaped = X_test_3.reshape((X_test_3.shape[0], 28, 28))\n",
        "\n",
        "\n",
        "print(X_train_9_reshaped.shape)\n",
        "print(X_val_9_reshaped.shape)\n",
        "print(X_test_9_reshaped.shape)\n",
        "print(X_train_7_reshaped.shape)\n",
        "print(X_val_7_reshaped.shape)\n",
        "print(X_test_7_reshaped.shape)\n",
        "print(X_train_5_reshaped.shape)\n",
        "print(X_val_5_reshaped.shape)\n",
        "print(X_test_5_reshaped.shape)\n",
        "print(X_train_3_reshaped.shape)\n",
        "print(X_val_3_reshaped.shape)\n",
        "print(X_test_3_reshaped.shape)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11893, 28, 28)\n",
            "(1487, 28, 28)\n",
            "(1487, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hmu9ilxUxqu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5qZfLDjIeFL",
        "outputId": "7c831258-d1e4-45e3-ae97-ad8c843c5f94"
      },
      "source": [
        "image_shape1 = (32, 32, 1)\n",
        "# this preporcessing is required by all resent networks and it's easy to apply.\n",
        "def preprocess(imgs, image_shape):\n",
        "  # apply resent preprocessing\n",
        "  shape = imgs.shape\n",
        "  x = tf.keras.applications.resnet50.preprocess_input(imgs)\n",
        "  # add an additional channel\n",
        "  x = x.reshape(shape + (1, ))\n",
        "  # resize the image\n",
        "  x = tf.image.resize(x, image_shape[:-1])\n",
        "  # normalize image pixels\n",
        "  x /= 255.0\n",
        "  return x\n",
        "\n",
        "print(X_train_9.shape)\n",
        "print(X_test_9.shape)\n",
        "\n",
        "x_train_use_9 = preprocess(X_train_9_reshaped, image_shape1)\n",
        "x_test_use_9 = preprocess(X_test_9_reshaped, image_shape1)\n",
        "x_val_use_9 = preprocess(X_val_9_reshaped, image_shape1)\n",
        "x_train_use_7 = preprocess(X_train_9_reshaped, image_shape1)\n",
        "x_test_use_7 = preprocess(X_test_9_reshaped, image_shape1)\n",
        "x_val_use_7 = preprocess(X_val_9_reshaped, image_shape1)\n",
        "x_train_use_5 = preprocess(X_train_9_reshaped, image_shape1)\n",
        "x_test_use_5 = preprocess(X_test_9_reshaped, image_shape1)\n",
        "x_val_use_5 = preprocess(X_val_9_reshaped, image_shape1)\n",
        "x_train_use_3 = preprocess(X_train_9_reshaped, image_shape1)\n",
        "x_test_use_3 = preprocess(X_test_9_reshaped, image_shape1)\n",
        "x_val_use_3 = preprocess(X_val_9_reshaped, image_shape1)\n",
        "# x_train = tf.image.resize(preprocess(x_train).reshape(x_train.shape + (1,) ), image_shape[:-1]) / 255.0\n",
        "# x_test  = tf.image.resize(preprocess(x_test).reshape(x_test.shape + (1,) ), image_shape[:-1]) / 255.0\n",
        "\n",
        "print(x_train_use_9.shape)\n",
        "print(x_test_use_9.shape)\n",
        "print(x_train_use_7.shape)\n",
        "print(x_test_use_7.shape)\n",
        "print(x_train_use_5.shape)\n",
        "print(x_test_use_5.shape)\n",
        "print(x_train_use_3.shape)\n",
        "print(x_test_use_3.shape)\n",
        "print(x_val_use_9.shape)\n",
        "print(x_val_use_7.shape)\n",
        "print(x_val_use_5.shape)\n",
        "print(x_val_use_3.shape)\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11893, 784)\n",
            "(1487, 784)\n",
            "(11893, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(11893, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(11893, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(11893, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n",
            "(1487, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXctiwNTJ4kV",
        "outputId": "1ada0c48-28a2-4b79-a6da-ace9ec5bfab1"
      },
      "source": [
        "# load trained model\n",
        "# we extract the network\n",
        "image_shape = (32, 32, 1)\n",
        "baseline_model_a = tf.keras.applications.resnet50.ResNet50(\n",
        "  # weights='imagenet',\n",
        "  weights=None,\n",
        "  include_top=True,\n",
        "  input_shape=image_shape\n",
        ")\n",
        "\n",
        "baseline_model_a.summary()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 38, 38, 1)    0           ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 16, 16, 64)   3200        ['conv1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 16, 16, 64)   256         ['conv1_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 16, 16, 64)   0           ['conv1_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 18, 18, 64)   0           ['conv1_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 8, 8, 64)     0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 8, 8, 64)     4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 8, 8, 256)    16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 8, 8, 256)    0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 4, 4, 128)    32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 4, 4, 512)    131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 4, 4, 512)    0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 4, 4, 512)    0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 2, 2, 256)    131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 2, 2, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                                                  'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_out[0][0]',       \n",
            "                                                                  'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block2_out[0][0]',       \n",
            "                                                                  'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block3_out[0][0]',       \n",
            "                                                                  'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block4_out[0][0]',       \n",
            "                                                                  'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block5_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block5_out[0][0]',       \n",
            "                                                                  'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block6_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 1, 1, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 1, 1, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " predictions (Dense)            (None, 1000)         2049000     ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,630,440\n",
            "Trainable params: 25,577,320\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs_HNTstJ4gf",
        "outputId": "23e4a980-54b6-4c83-a84d-cefb7a61c161"
      },
      "source": [
        "#base_model compile and fit model 9\n",
        "\n",
        "add_model = tf.keras.Sequential()\n",
        "add_model.add(baseline_model_a)\n",
        "add_model.add(tf.keras.layers.Dropout(0.2))\n",
        "add_model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "baseline_model_ax = add_model\n",
        "baseline_model_ax.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])\n",
        "\n",
        "history_base_model = baseline_model_ax.fit(x=x_train_use,y=y_train, validation_data=(x_test_use, y_test), epochs=100)\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "372/372 [==============================] - 17s 37ms/step - loss: 0.4537 - accuracy: 0.8806 - val_loss: 0.8368 - val_accuracy: 0.4533\n",
            "Epoch 2/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.2723 - accuracy: 0.9020 - val_loss: 0.9702 - val_accuracy: 0.4633\n",
            "Epoch 3/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.2168 - accuracy: 0.9040 - val_loss: 0.9786 - val_accuracy: 0.5017\n",
            "Epoch 4/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1996 - accuracy: 0.8994 - val_loss: 1.2742 - val_accuracy: 0.4492\n",
            "Epoch 5/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1826 - accuracy: 0.9047 - val_loss: 0.9854 - val_accuracy: 0.5555\n",
            "Epoch 6/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1781 - accuracy: 0.9029 - val_loss: 1.3718 - val_accuracy: 0.4512\n",
            "Epoch 7/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1701 - accuracy: 0.9064 - val_loss: 1.2770 - val_accuracy: 0.4882\n",
            "Epoch 8/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1679 - accuracy: 0.9031 - val_loss: 1.3901 - val_accuracy: 0.4573\n",
            "Epoch 9/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1648 - accuracy: 0.9048 - val_loss: 0.9256 - val_accuracy: 0.6086\n",
            "Epoch 10/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1618 - accuracy: 0.9036 - val_loss: 1.0458 - val_accuracy: 0.5992\n",
            "Epoch 11/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1576 - accuracy: 0.9067 - val_loss: 1.2547 - val_accuracy: 0.4728\n",
            "Epoch 12/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1575 - accuracy: 0.9070 - val_loss: 0.9653 - val_accuracy: 0.6429\n",
            "Epoch 13/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1528 - accuracy: 0.9018 - val_loss: 0.7159 - val_accuracy: 0.7209\n",
            "Epoch 14/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1536 - accuracy: 0.9022 - val_loss: 1.0288 - val_accuracy: 0.6342\n",
            "Epoch 15/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1528 - accuracy: 0.9049 - val_loss: 0.6883 - val_accuracy: 0.7384\n",
            "Epoch 16/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1536 - accuracy: 0.9041 - val_loss: 0.4864 - val_accuracy: 0.8151\n",
            "Epoch 17/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1581 - accuracy: 0.9029 - val_loss: 0.4582 - val_accuracy: 0.8184\n",
            "Epoch 18/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1512 - accuracy: 0.9079 - val_loss: 0.4636 - val_accuracy: 0.8554\n",
            "Epoch 19/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1516 - accuracy: 0.9041 - val_loss: 1.0950 - val_accuracy: 0.6046\n",
            "Epoch 20/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1512 - accuracy: 0.9036 - val_loss: 1.2761 - val_accuracy: 0.5696\n",
            "Epoch 21/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1486 - accuracy: 0.9053 - val_loss: 0.9324 - val_accuracy: 0.7034\n",
            "Epoch 22/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1505 - accuracy: 0.9044 - val_loss: 1.8279 - val_accuracy: 0.4640\n",
            "Epoch 23/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1476 - accuracy: 0.9056 - val_loss: 1.8913 - val_accuracy: 0.4660\n",
            "Epoch 24/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1518 - accuracy: 0.9025 - val_loss: 1.7020 - val_accuracy: 0.4849\n",
            "Epoch 25/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1486 - accuracy: 0.9014 - val_loss: 1.7914 - val_accuracy: 0.4869\n",
            "Epoch 26/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1549 - accuracy: 0.8998 - val_loss: 1.8507 - val_accuracy: 0.4862\n",
            "Epoch 27/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1457 - accuracy: 0.9051 - val_loss: 1.6880 - val_accuracy: 0.5084\n",
            "Epoch 28/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1452 - accuracy: 0.9093 - val_loss: 1.4426 - val_accuracy: 0.5757\n",
            "Epoch 29/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1460 - accuracy: 0.9046 - val_loss: 1.1126 - val_accuracy: 0.6644\n",
            "Epoch 30/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1482 - accuracy: 0.9019 - val_loss: 1.3723 - val_accuracy: 0.5716\n",
            "Epoch 31/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1472 - accuracy: 0.9089 - val_loss: 1.5313 - val_accuracy: 0.5380\n",
            "Epoch 32/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1426 - accuracy: 0.9075 - val_loss: 1.4235 - val_accuracy: 0.5783\n",
            "Epoch 33/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1444 - accuracy: 0.9046 - val_loss: 1.1697 - val_accuracy: 0.6516\n",
            "Epoch 34/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1446 - accuracy: 0.9055 - val_loss: 1.1922 - val_accuracy: 0.6523\n",
            "Epoch 35/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1425 - accuracy: 0.9073 - val_loss: 1.5141 - val_accuracy: 0.5831\n",
            "Epoch 36/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1427 - accuracy: 0.9073 - val_loss: 1.7137 - val_accuracy: 0.5306\n",
            "Epoch 37/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1433 - accuracy: 0.9106 - val_loss: 1.6535 - val_accuracy: 0.5508\n",
            "Epoch 38/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1497 - accuracy: 0.9060 - val_loss: 1.9567 - val_accuracy: 0.4506\n",
            "Epoch 39/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1461 - accuracy: 0.9030 - val_loss: 2.0569 - val_accuracy: 0.4492\n",
            "Epoch 40/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1454 - accuracy: 0.9043 - val_loss: 1.6096 - val_accuracy: 0.4929\n",
            "Epoch 41/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1413 - accuracy: 0.9067 - val_loss: 1.8698 - val_accuracy: 0.4667\n",
            "Epoch 42/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1473 - accuracy: 0.9066 - val_loss: 1.9435 - val_accuracy: 0.4660\n",
            "Epoch 43/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1422 - accuracy: 0.9076 - val_loss: 1.9430 - val_accuracy: 0.4781\n",
            "Epoch 44/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1445 - accuracy: 0.9025 - val_loss: 2.0530 - val_accuracy: 0.4707\n",
            "Epoch 45/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1432 - accuracy: 0.9072 - val_loss: 1.9849 - val_accuracy: 0.4815\n",
            "Epoch 46/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1435 - accuracy: 0.9056 - val_loss: 2.0093 - val_accuracy: 0.4822\n",
            "Epoch 47/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1421 - accuracy: 0.9045 - val_loss: 2.0108 - val_accuracy: 0.4822\n",
            "Epoch 48/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1464 - accuracy: 0.9025 - val_loss: 2.0548 - val_accuracy: 0.4788\n",
            "Epoch 49/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1381 - accuracy: 0.9091 - val_loss: 2.0840 - val_accuracy: 0.4748\n",
            "Epoch 50/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1425 - accuracy: 0.9047 - val_loss: 2.1251 - val_accuracy: 0.4721\n",
            "Epoch 51/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1415 - accuracy: 0.9059 - val_loss: 2.0509 - val_accuracy: 0.4835\n",
            "Epoch 52/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1460 - accuracy: 0.9051 - val_loss: 1.4314 - val_accuracy: 0.5952\n",
            "Epoch 53/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1436 - accuracy: 0.9076 - val_loss: 1.6808 - val_accuracy: 0.5440\n",
            "Epoch 54/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1429 - accuracy: 0.9067 - val_loss: 1.7090 - val_accuracy: 0.5481\n",
            "Epoch 55/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1404 - accuracy: 0.9091 - val_loss: 1.6666 - val_accuracy: 0.5642\n",
            "Epoch 56/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1453 - accuracy: 0.9024 - val_loss: 1.8020 - val_accuracy: 0.5340\n",
            "Epoch 57/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1415 - accuracy: 0.9050 - val_loss: 1.7994 - val_accuracy: 0.5420\n",
            "Epoch 58/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1398 - accuracy: 0.9045 - val_loss: 1.8349 - val_accuracy: 0.5319\n",
            "Epoch 59/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1411 - accuracy: 0.9094 - val_loss: 1.8484 - val_accuracy: 0.5286\n",
            "Epoch 60/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1427 - accuracy: 0.9073 - val_loss: 0.9548 - val_accuracy: 0.7357\n",
            "Epoch 61/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1442 - accuracy: 0.9030 - val_loss: 1.3378 - val_accuracy: 0.6308\n",
            "Epoch 62/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1440 - accuracy: 0.9042 - val_loss: 2.1698 - val_accuracy: 0.4694\n",
            "Epoch 63/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1438 - accuracy: 0.9040 - val_loss: 2.1180 - val_accuracy: 0.4687\n",
            "Epoch 64/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1371 - accuracy: 0.9094 - val_loss: 2.0426 - val_accuracy: 0.4707\n",
            "Epoch 65/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1393 - accuracy: 0.9070 - val_loss: 1.5420 - val_accuracy: 0.6005\n",
            "Epoch 66/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1423 - accuracy: 0.9044 - val_loss: 1.5076 - val_accuracy: 0.6140\n",
            "Epoch 67/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1404 - accuracy: 0.9031 - val_loss: 1.5464 - val_accuracy: 0.6079\n",
            "Epoch 68/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1426 - accuracy: 0.9042 - val_loss: 1.5565 - val_accuracy: 0.6073\n",
            "Epoch 69/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1420 - accuracy: 0.9046 - val_loss: 1.5601 - val_accuracy: 0.6073\n",
            "Epoch 70/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1426 - accuracy: 0.9050 - val_loss: 1.4353 - val_accuracy: 0.6429\n",
            "Epoch 71/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1433 - accuracy: 0.9063 - val_loss: 1.4503 - val_accuracy: 0.6382\n",
            "Epoch 72/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1417 - accuracy: 0.9059 - val_loss: 1.4840 - val_accuracy: 0.6335\n",
            "Epoch 73/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1463 - accuracy: 0.9034 - val_loss: 1.4756 - val_accuracy: 0.6355\n",
            "Epoch 74/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1423 - accuracy: 0.9026 - val_loss: 1.5023 - val_accuracy: 0.6321\n",
            "Epoch 75/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1405 - accuracy: 0.9040 - val_loss: 1.5314 - val_accuracy: 0.6281\n",
            "Epoch 76/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1368 - accuracy: 0.9046 - val_loss: 1.5236 - val_accuracy: 0.6308\n",
            "Epoch 77/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1396 - accuracy: 0.9073 - val_loss: 1.5468 - val_accuracy: 0.6281\n",
            "Epoch 78/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1366 - accuracy: 0.9103 - val_loss: 1.5195 - val_accuracy: 0.6308\n",
            "Epoch 79/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1411 - accuracy: 0.9062 - val_loss: 1.5588 - val_accuracy: 0.6254\n",
            "Epoch 80/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1345 - accuracy: 0.9091 - val_loss: 1.5640 - val_accuracy: 0.6247\n",
            "Epoch 81/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1372 - accuracy: 0.9091 - val_loss: 1.5785 - val_accuracy: 0.6214\n",
            "Epoch 82/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1414 - accuracy: 0.9031 - val_loss: 1.5965 - val_accuracy: 0.6194\n",
            "Epoch 83/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1408 - accuracy: 0.9047 - val_loss: 1.5870 - val_accuracy: 0.6221\n",
            "Epoch 84/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1399 - accuracy: 0.9095 - val_loss: 1.6277 - val_accuracy: 0.6126\n",
            "Epoch 85/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1359 - accuracy: 0.9089 - val_loss: 1.5924 - val_accuracy: 0.6207\n",
            "Epoch 86/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1422 - accuracy: 0.9060 - val_loss: 1.6006 - val_accuracy: 0.6200\n",
            "Epoch 87/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1451 - accuracy: 0.9014 - val_loss: 1.6464 - val_accuracy: 0.6113\n",
            "Epoch 88/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1420 - accuracy: 0.9055 - val_loss: 1.6025 - val_accuracy: 0.6227\n",
            "Epoch 89/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1447 - accuracy: 0.9018 - val_loss: 0.8201 - val_accuracy: 0.7774\n",
            "Epoch 90/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1470 - accuracy: 0.8999 - val_loss: 1.3653 - val_accuracy: 0.6678\n",
            "Epoch 91/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1402 - accuracy: 0.9043 - val_loss: 1.5502 - val_accuracy: 0.6281\n",
            "Epoch 92/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1419 - accuracy: 0.9064 - val_loss: 1.5162 - val_accuracy: 0.6342\n",
            "Epoch 93/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1371 - accuracy: 0.9076 - val_loss: 1.5804 - val_accuracy: 0.6221\n",
            "Epoch 94/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1392 - accuracy: 0.9084 - val_loss: 1.5574 - val_accuracy: 0.6288\n",
            "Epoch 95/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1389 - accuracy: 0.9059 - val_loss: 1.6268 - val_accuracy: 0.6113\n",
            "Epoch 96/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1417 - accuracy: 0.9039 - val_loss: 1.7452 - val_accuracy: 0.5844\n",
            "Epoch 97/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1406 - accuracy: 0.9042 - val_loss: 1.7806 - val_accuracy: 0.5757\n",
            "Epoch 98/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1414 - accuracy: 0.9066 - val_loss: 2.1949 - val_accuracy: 0.4526\n",
            "Epoch 99/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1433 - accuracy: 0.9049 - val_loss: 1.6821 - val_accuracy: 0.5474\n",
            "Epoch 100/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1406 - accuracy: 0.9089 - val_loss: 1.5660 - val_accuracy: 0.5898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWzLCSmHJ4bc",
        "outputId": "c810af5b-49a3-4307-9e0e-4dd97624aa20"
      },
      "source": [
        "#base_model compile and fit model 7\n",
        "\n",
        "history_base_model_7 = baseline_model_ax.fit(x=x_train_use_7,y=y_train, validation_data=(x_test_use_7, y_test), epochs=100)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1389 - accuracy: 0.9094 - val_loss: 1.5469 - val_accuracy: 0.5562\n",
            "Epoch 2/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1446 - accuracy: 0.9027 - val_loss: 1.4837 - val_accuracy: 0.5844\n",
            "Epoch 3/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1443 - accuracy: 0.9030 - val_loss: 1.5162 - val_accuracy: 0.5804\n",
            "Epoch 4/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1377 - accuracy: 0.9059 - val_loss: 1.7445 - val_accuracy: 0.5481\n",
            "Epoch 5/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1415 - accuracy: 0.9040 - val_loss: 1.6725 - val_accuracy: 0.5750\n",
            "Epoch 6/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1420 - accuracy: 0.9052 - val_loss: 1.8219 - val_accuracy: 0.5514\n",
            "Epoch 7/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1429 - accuracy: 0.9033 - val_loss: 0.7320 - val_accuracy: 0.7700\n",
            "Epoch 8/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1413 - accuracy: 0.9053 - val_loss: 0.7375 - val_accuracy: 0.7734\n",
            "Epoch 9/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1436 - accuracy: 0.9015 - val_loss: 0.8154 - val_accuracy: 0.7613\n",
            "Epoch 10/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1376 - accuracy: 0.9072 - val_loss: 0.9189 - val_accuracy: 0.7485\n",
            "Epoch 11/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1398 - accuracy: 0.9055 - val_loss: 0.5163 - val_accuracy: 0.8299\n",
            "Epoch 12/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1384 - accuracy: 0.9069 - val_loss: 0.5989 - val_accuracy: 0.8137\n",
            "Epoch 13/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1404 - accuracy: 0.9055 - val_loss: 0.6540 - val_accuracy: 0.8117\n",
            "Epoch 14/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1430 - accuracy: 0.9046 - val_loss: 0.6968 - val_accuracy: 0.8070\n",
            "Epoch 15/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1404 - accuracy: 0.9051 - val_loss: 0.6785 - val_accuracy: 0.8110\n",
            "Epoch 16/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1399 - accuracy: 0.9043 - val_loss: 0.6804 - val_accuracy: 0.8110\n",
            "Epoch 17/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1411 - accuracy: 0.9052 - val_loss: 0.6947 - val_accuracy: 0.8056\n",
            "Epoch 18/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1405 - accuracy: 0.9062 - val_loss: 0.7311 - val_accuracy: 0.8016\n",
            "Epoch 19/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1375 - accuracy: 0.9087 - val_loss: 0.5605 - val_accuracy: 0.8373\n",
            "Epoch 20/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1434 - accuracy: 0.9041 - val_loss: 0.5563 - val_accuracy: 0.8379\n",
            "Epoch 21/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1384 - accuracy: 0.9077 - val_loss: 0.5376 - val_accuracy: 0.8453\n",
            "Epoch 22/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1410 - accuracy: 0.9086 - val_loss: 0.5214 - val_accuracy: 0.8494\n",
            "Epoch 23/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1403 - accuracy: 0.9057 - val_loss: 0.4773 - val_accuracy: 0.8568\n",
            "Epoch 24/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1415 - accuracy: 0.9073 - val_loss: 0.5245 - val_accuracy: 0.8480\n",
            "Epoch 25/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1354 - accuracy: 0.9100 - val_loss: 0.5416 - val_accuracy: 0.8453\n",
            "Epoch 26/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1420 - accuracy: 0.9069 - val_loss: 0.5106 - val_accuracy: 0.8467\n",
            "Epoch 27/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1430 - accuracy: 0.9062 - val_loss: 0.4723 - val_accuracy: 0.8588\n",
            "Epoch 28/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1429 - accuracy: 0.9060 - val_loss: 0.4650 - val_accuracy: 0.8615\n",
            "Epoch 29/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1394 - accuracy: 0.9058 - val_loss: 0.5126 - val_accuracy: 0.8514\n",
            "Epoch 30/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1360 - accuracy: 0.9057 - val_loss: 0.5272 - val_accuracy: 0.8480\n",
            "Epoch 31/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1381 - accuracy: 0.9084 - val_loss: 0.3265 - val_accuracy: 0.8870\n",
            "Epoch 32/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1417 - accuracy: 0.9053 - val_loss: 0.3400 - val_accuracy: 0.8837\n",
            "Epoch 33/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1363 - accuracy: 0.9107 - val_loss: 0.4355 - val_accuracy: 0.8594\n",
            "Epoch 34/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1351 - accuracy: 0.9111 - val_loss: 0.4387 - val_accuracy: 0.8561\n",
            "Epoch 35/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1441 - accuracy: 0.9020 - val_loss: 0.4353 - val_accuracy: 0.8568\n",
            "Epoch 36/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1400 - accuracy: 0.9062 - val_loss: 0.4515 - val_accuracy: 0.8568\n",
            "Epoch 37/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1391 - accuracy: 0.9083 - val_loss: 0.4533 - val_accuracy: 0.8554\n",
            "Epoch 38/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1384 - accuracy: 0.9088 - val_loss: 0.4677 - val_accuracy: 0.8521\n",
            "Epoch 39/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1405 - accuracy: 0.9043 - val_loss: 0.4862 - val_accuracy: 0.8460\n",
            "Epoch 40/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1404 - accuracy: 0.9040 - val_loss: 0.5239 - val_accuracy: 0.8420\n",
            "Epoch 41/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1406 - accuracy: 0.9043 - val_loss: 0.5549 - val_accuracy: 0.8366\n",
            "Epoch 42/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1430 - accuracy: 0.9046 - val_loss: 0.5677 - val_accuracy: 0.8339\n",
            "Epoch 43/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1377 - accuracy: 0.9082 - val_loss: 0.5966 - val_accuracy: 0.8285\n",
            "Epoch 44/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1410 - accuracy: 0.9026 - val_loss: 0.6042 - val_accuracy: 0.8245\n",
            "Epoch 45/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1448 - accuracy: 0.9025 - val_loss: 0.6331 - val_accuracy: 0.8178\n",
            "Epoch 46/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1396 - accuracy: 0.9088 - val_loss: 0.6360 - val_accuracy: 0.8184\n",
            "Epoch 47/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1383 - accuracy: 0.9076 - val_loss: 0.6522 - val_accuracy: 0.8191\n",
            "Epoch 48/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1426 - accuracy: 0.9058 - val_loss: 0.6270 - val_accuracy: 0.8231\n",
            "Epoch 49/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1370 - accuracy: 0.9086 - val_loss: 0.6045 - val_accuracy: 0.8299\n",
            "Epoch 50/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1434 - accuracy: 0.9045 - val_loss: 0.6440 - val_accuracy: 0.8184\n",
            "Epoch 51/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1398 - accuracy: 0.9054 - val_loss: 0.6511 - val_accuracy: 0.8157\n",
            "Epoch 52/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1379 - accuracy: 0.9057 - val_loss: 0.6349 - val_accuracy: 0.8238\n",
            "Epoch 53/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1391 - accuracy: 0.9094 - val_loss: 0.6391 - val_accuracy: 0.8191\n",
            "Epoch 54/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1375 - accuracy: 0.9112 - val_loss: 0.6266 - val_accuracy: 0.8238\n",
            "Epoch 55/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1391 - accuracy: 0.9065 - val_loss: 0.6584 - val_accuracy: 0.8171\n",
            "Epoch 56/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1419 - accuracy: 0.9034 - val_loss: 0.6448 - val_accuracy: 0.8204\n",
            "Epoch 57/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1412 - accuracy: 0.9068 - val_loss: 0.6332 - val_accuracy: 0.8245\n",
            "Epoch 58/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1418 - accuracy: 0.9070 - val_loss: 0.6396 - val_accuracy: 0.8238\n",
            "Epoch 59/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1407 - accuracy: 0.9059 - val_loss: 0.6737 - val_accuracy: 0.8151\n",
            "Epoch 60/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1478 - accuracy: 0.9010 - val_loss: 0.6704 - val_accuracy: 0.8178\n",
            "Epoch 61/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1380 - accuracy: 0.9064 - val_loss: 0.6684 - val_accuracy: 0.8157\n",
            "Epoch 62/100\n",
            "372/372 [==============================] - 13s 34ms/step - loss: 0.1389 - accuracy: 0.9070 - val_loss: 0.6539 - val_accuracy: 0.8252\n",
            "Epoch 63/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1421 - accuracy: 0.9024 - val_loss: 0.5376 - val_accuracy: 0.8561\n",
            "Epoch 64/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1381 - accuracy: 0.9043 - val_loss: 0.5787 - val_accuracy: 0.8514\n",
            "Epoch 65/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1396 - accuracy: 0.9100 - val_loss: 0.5893 - val_accuracy: 0.8487\n",
            "Epoch 66/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1379 - accuracy: 0.9049 - val_loss: 0.5552 - val_accuracy: 0.8554\n",
            "Epoch 67/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1380 - accuracy: 0.9073 - val_loss: 0.5892 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1353 - accuracy: 0.9097 - val_loss: 0.5861 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1424 - accuracy: 0.9026 - val_loss: 0.5826 - val_accuracy: 0.8521\n",
            "Epoch 70/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1384 - accuracy: 0.9081 - val_loss: 0.5915 - val_accuracy: 0.8507\n",
            "Epoch 71/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1390 - accuracy: 0.9038 - val_loss: 0.6121 - val_accuracy: 0.8460\n",
            "Epoch 72/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1383 - accuracy: 0.9080 - val_loss: 0.7751 - val_accuracy: 0.8144\n",
            "Epoch 73/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1379 - accuracy: 0.9058 - val_loss: 0.6220 - val_accuracy: 0.8473\n",
            "Epoch 74/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1342 - accuracy: 0.9129 - val_loss: 0.9158 - val_accuracy: 0.7599\n",
            "Epoch 75/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1417 - accuracy: 0.9020 - val_loss: 0.9346 - val_accuracy: 0.7693\n",
            "Epoch 76/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1394 - accuracy: 0.9058 - val_loss: 1.0967 - val_accuracy: 0.7512\n",
            "Epoch 77/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1426 - accuracy: 0.9051 - val_loss: 1.1259 - val_accuracy: 0.7471\n",
            "Epoch 78/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1403 - accuracy: 0.9040 - val_loss: 1.0983 - val_accuracy: 0.7518\n",
            "Epoch 79/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1414 - accuracy: 0.9059 - val_loss: 1.2557 - val_accuracy: 0.7223\n",
            "Epoch 80/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1411 - accuracy: 0.9028 - val_loss: 1.3059 - val_accuracy: 0.7189\n",
            "Epoch 81/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1410 - accuracy: 0.9069 - val_loss: 1.3098 - val_accuracy: 0.7128\n",
            "Epoch 82/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1444 - accuracy: 0.9025 - val_loss: 1.2871 - val_accuracy: 0.7176\n",
            "Epoch 83/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1388 - accuracy: 0.9062 - val_loss: 1.2888 - val_accuracy: 0.7196\n",
            "Epoch 84/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1428 - accuracy: 0.9046 - val_loss: 1.2687 - val_accuracy: 0.7216\n",
            "Epoch 85/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1373 - accuracy: 0.9081 - val_loss: 1.2875 - val_accuracy: 0.7176\n",
            "Epoch 86/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1412 - accuracy: 0.9053 - val_loss: 1.2434 - val_accuracy: 0.7256\n",
            "Epoch 87/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1374 - accuracy: 0.9076 - val_loss: 1.2825 - val_accuracy: 0.7223\n",
            "Epoch 88/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1424 - accuracy: 0.9039 - val_loss: 1.3056 - val_accuracy: 0.7196\n",
            "Epoch 89/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1396 - accuracy: 0.9059 - val_loss: 1.3099 - val_accuracy: 0.7169\n",
            "Epoch 90/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1392 - accuracy: 0.9069 - val_loss: 1.1410 - val_accuracy: 0.7505\n",
            "Epoch 91/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1415 - accuracy: 0.9048 - val_loss: 1.1583 - val_accuracy: 0.7471\n",
            "Epoch 92/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1433 - accuracy: 0.9062 - val_loss: 1.1456 - val_accuracy: 0.7512\n",
            "Epoch 93/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1379 - accuracy: 0.9068 - val_loss: 1.0778 - val_accuracy: 0.7653\n",
            "Epoch 94/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1423 - accuracy: 0.9059 - val_loss: 1.0469 - val_accuracy: 0.7720\n",
            "Epoch 95/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1413 - accuracy: 0.9044 - val_loss: 0.9908 - val_accuracy: 0.7801\n",
            "Epoch 96/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1435 - accuracy: 0.8998 - val_loss: 1.0304 - val_accuracy: 0.7734\n",
            "Epoch 97/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1394 - accuracy: 0.9072 - val_loss: 1.0650 - val_accuracy: 0.7687\n",
            "Epoch 98/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1372 - accuracy: 0.9078 - val_loss: 1.0747 - val_accuracy: 0.7666\n",
            "Epoch 99/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1406 - accuracy: 0.9058 - val_loss: 1.1056 - val_accuracy: 0.7646\n",
            "Epoch 100/100\n",
            "372/372 [==============================] - 13s 35ms/step - loss: 0.1359 - accuracy: 0.9097 - val_loss: 1.0964 - val_accuracy: 0.7646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kI_rwZcXRw4"
      },
      "source": [
        "#base_model compile and fit model 5\n",
        "\n",
        "history_base_model_5 = baseline_model_ax.fit(x=x_train_use_5,y=y_train, validation_data=(x_test_use_5, y_test), epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bVfJtwNXXLt"
      },
      "source": [
        "#base_model compile and fit model 3\n",
        "\n",
        "history_base_model_3 = baseline_model_ax.fit(x=x_train_use_3,y=y_train, validation_data=(x_test_use_3, y_test), epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCbM4gttKA6_"
      },
      "source": [
        "Pauls Part End"
      ]
    }
  ]
}