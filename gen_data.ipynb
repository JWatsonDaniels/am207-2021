{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Classification Can Magnify Disparities Across Groups\n",
    "\n",
    "\n",
    "### AM207: Advanced Scientific Computing\n",
    "\n",
    "#### Team: Jamelle Watson-Daniels, Shirley Wang, Bridger Ruyle, Paul Tembo\n",
    "\n",
    "#### Paper: https://arxiv.org/pdf/2010.14134.pdfODOS: \n",
    "\n",
    "Minimal goal: Investigate how selective classification affects group accuracy on a synthetic `mnist` dataset\n",
    "- Generate synthetic dataset from `mnist`\n",
    "- Train empirical risk minimization model (minimize loss - Logistic regression i.e. ResNet)    \n",
    "- Calculate average coverage\n",
    "- Calculate group coverage\n",
    "- Calculate selective accuracy\n",
    "- Plot average coverage vs group coverage for `mnist` dataset\n",
    "- Plot average coverage vs selective accuracy for `mnist` dataset\n",
    "\n",
    "If there is time goal: Show that selective accuracies on average and on the worst-group are determined by their respective margin distributions\n",
    "- Calculate margin\n",
    "- Calculate worst-group density\n",
    "- Calculate Average density\n",
    "- Plot margin (-10,+10) versus worst-group density\n",
    "- Plot margin (-10,+10) versus Average density\n",
    "\n",
    "Possible extensions: What are the characteristics of this dataset that are imbalanced and contain spurious correlations? We see that in all these datasets, the \"worst group\" is underrepresented as in there are much fewer examples for the \"worst group\" category. Notice that the datasets considered in this paper do not seem immediately relevant in terms of the real world implications of disparities, we wonder how we might examine this using toy examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset description\n",
    "\"Models have been shown to latch onto spurious correlations between labels and demographic attributes such as race and gender (Buolamwini & Gebru, 2018; Joshi et al., 2018), and we\n",
    "study this on the CelebA dataset (Liu et al., 2015). Following Sagawa et al. (2020), we consider\n",
    "the task of classifying hair color, which is spuriously correlated with the gender. Concretely, inputs\n",
    "are celebrity face images, labels are hair color Y = {blond, non-blond}, and spurious attributes are\n",
    "gender, A = {male, female}, with blondness associated with being female. Of the four groups,\n",
    "blond males are the smallest group, with only 1,387 examples out of 162,770 training examples, and\n",
    "they tend to be the worst group empirically. We use the official train-val-split of the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gdown\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
